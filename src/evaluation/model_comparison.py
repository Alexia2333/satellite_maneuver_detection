# src/evaluation/model_comparison.py
"""
Êó†ÁõëÁù£Ê®°ÂûãÁ´ûËµõÊ°ÜÊû∂ (Model Bakeoff Framework)

Ëøô‰∏™Ê®°ÂùóÂÆûÁé∞‰∫ÜÊñπÊ°à‰∏≠Êé®ËçêÁöÑ"Ê®°ÂûãÁ´ûËµõ"Ê¶ÇÂøµÔºåÁî®‰∫éÂÆ¢ËßÇÂú∞ËØÑ‰º∞ÂíåÊØîËæÉ
‰∏çÂêåÊó†ÁõëÁù£Êú∫Âä®Ê£ÄÊµãÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇÈÄöËøá‰ΩøÁî®2020-2022Âπ¥ÁöÑÈ£é‰∫ë4AÊú∫Âä®Êó•Âøó
‰Ωú‰∏∫Âú∞Èù¢ÁúüÂÆûÂÄºÔºåÊàë‰ª¨ÂèØ‰ª•ÂØπÂêÑÁßçÊó†ÁõëÁù£ÁÆóÊ≥ïËøõË°å‰∏•Ê†ºÁöÑÂü∫ÂáÜÊµãËØï„ÄÇ

Key Features:
- Áªü‰∏ÄÁöÑÊ®°ÂûãÊé•Âè£ÂíåËØÑ‰º∞Ê°ÜÊû∂
- Ê†áÂáÜÂåñÁöÑÊÄßËÉΩÊåáÊ†áËÆ°ÁÆó
- Ëá™Âä®ÂåñÁöÑÊ®°ÂûãÈÄâÊã©ÂíåÊéíÂêç
- ËØ¶ÁªÜÁöÑÊÄßËÉΩÂàÜÊûêÊä•Âëä
- ÂèØËßÜÂåñÁöÑÁªìÊûúÂØπÊØî
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Protocol
from datetime import datetime, timedelta
from dataclasses import dataclass
from abc import ABC, abstractmethod

import warnings
from sklearn.metrics import (
    precision_score, recall_score, f1_score, roc_auc_score,
    precision_recall_curve, roc_curve, auc, confusion_matrix
)

# ÂØºÂÖ•Êàë‰ª¨ÁöÑÊ£ÄÊµãÂô®
from src.models.unsupervised.particle_filter import ParticleFilterDetector
from src.models.unsupervised.lstm_vae import LSTMVAEDetector

@dataclass
class ModelPerformance:
    """Ê®°ÂûãÊÄßËÉΩÁªìÊûúÊï∞ÊçÆÁ±ª"""
    model_name: str
    precision: float
    recall: float
    f1_score: float
    auc_pr: float  # PRÊõ≤Á∫ø‰∏ãÈù¢ÁßØ
    auc_roc: float  # ROCÊõ≤Á∫ø‰∏ãÈù¢ÁßØ
    true_positives: int
    false_positives: int
    false_negatives: int
    true_negatives: int
    total_detections: int
    total_true_events: int
    anomaly_scores: np.ndarray
    detection_timestamps: List[datetime]
    training_time: float
    detection_time: float
    model_params: Dict[str, Any]

class UnsupervisedDetector(Protocol):
    """Êó†ÁõëÁù£Ê£ÄÊµãÂô®ÁöÑÁªü‰∏ÄÊé•Âè£ÂçèËÆÆ"""
    
    def fit(self, train_data: pd.DataFrame, 
            maneuver_labels: Optional[pd.Series] = None) -> None:
        """ËÆ≠ÁªÉÊ£ÄÊµãÂô®"""
        ...
    
    def detect_anomalies(self, test_data: pd.DataFrame) -> Tuple[List[datetime], np.ndarray]:
        """Ê£ÄÊµãÂºÇÂ∏∏ÔºåËøîÂõûÂºÇÂ∏∏Êó∂ÂàªÂíåÂºÇÂ∏∏ÂàÜÊï∞"""
        ...
    
    def get_model_summary(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊ®°ÂûãÊëòË¶Å"""
        ...

class ModelBakeoff:
    """
    Êó†ÁõëÁù£Ê®°ÂûãÁ´ûËµõÊ°ÜÊû∂
    
    Ëøô‰∏™Á±ªÁÆ°ÁêÜÂ§ö‰∏™Êó†ÁõëÁù£Ê£ÄÊµãÂô®ÁöÑËÆ≠ÁªÉ„ÄÅËØÑ‰º∞ÂíåÊØîËæÉÔºå
    Êèê‰æõÂÆ¢ËßÇÁöÑÊÄßËÉΩËØÑ‰º∞ÂíåÊ®°ÂûãÈÄâÊã©Âª∫ËÆÆ„ÄÇ
    """
    
    def __init__(self, 
                 tolerance_window: timedelta = timedelta(days=1),
                 min_time_between_events: timedelta = timedelta(hours=12)):
        """
        ÂàùÂßãÂåñÁ´ûËµõÊ°ÜÊû∂
        
        Args:
            tolerance_window: Ê£ÄÊµãÂÆπÂøçÁ™óÂè£ÔºàÊ£ÄÊµãÂú®ÁúüÂÆû‰∫ã‰ª∂ÂâçÂêéÊ≠§Êó∂Èó¥ÂÜÖÁÆóÊ≠£Á°ÆÔºâ
            min_time_between_events: ËøûÁª≠‰∫ã‰ª∂Èó¥ÊúÄÂ∞èÊó∂Èó¥Èó¥Èöî
        """
        self.tolerance_window = tolerance_window
        self.min_time_between_events = min_time_between_events
        
        # Ê≥®ÂÜåÁöÑÊ®°Âûã
        self.models: Dict[str, UnsupervisedDetector] = {}
        self.model_configs: Dict[str, Dict] = {}
        
        # ËØÑ‰º∞ÁªìÊûú
        self.results: Dict[str, ModelPerformance] = {}
        self.ground_truth_events: List[datetime] = []
        
        # Êï∞ÊçÆÈõÜ‰ø°ÊÅØ
        self.train_data: Optional[pd.DataFrame] = None
        self.test_data: Optional[pd.DataFrame] = None
        self.train_labels: Optional[pd.Series] = None
        self.test_labels: Optional[pd.Series] = None
        
        print("üèÅ Model Bakeoff Framework initialized")
    
    def register_model(self, name: str, model: UnsupervisedDetector, 
                      config: Optional[Dict] = None) -> None:
        """
        Ê≥®ÂÜåÂèÇËµõÊ®°Âûã
        
        Args:
            name: Ê®°ÂûãÂêçÁß∞
            model: Ê£ÄÊµãÂô®ÂÆû‰æã
            config: Ê®°ÂûãÈÖçÁΩÆ‰ø°ÊÅØ
        """
        self.models[name] = model
        self.model_configs[name] = config or {}
        print(f"‚úÖ Registered model: {name}")
    
    def setup_data(self, train_data: pd.DataFrame, test_data: pd.DataFrame,
                   train_labels: pd.Series, test_labels: pd.Series) -> None:
        """
        ËÆæÁΩÆËÆ≠ÁªÉÂíåÊµãËØïÊï∞ÊçÆ
        
        Args:
            train_data: ËÆ≠ÁªÉÊï∞ÊçÆ
            test_data: ÊµãËØïÊï∞ÊçÆ  
            train_labels: ËÆ≠ÁªÉÊ†áÁ≠æ
            test_labels: ÊµãËØïÊ†áÁ≠æ
        """
        self.train_data = train_data
        self.test_data = test_data
        self.train_labels = train_labels
        self.test_labels = test_labels
        
        # ÊèêÂèñÁúüÂÆû‰∫ã‰ª∂Êó∂Âàª
        self.ground_truth_events = test_labels[test_labels == 1].index.tolist()
        
        print(f"üìä Data setup complete:")
        print(f"   -> Training data: {len(train_data)} records")
        print(f"   -> Test data: {len(test_data)} records")
        print(f"   -> Ground truth events: {len(self.ground_truth_events)}")
    
    def run_competition(self, verbose: bool = True) -> Dict[str, ModelPerformance]:
        """
        ËøêË°åÊ®°ÂûãÁ´ûËµõ
        
        Args:
            verbose: ÊòØÂê¶ÊòæÁ§∫ËØ¶ÁªÜ‰ø°ÊÅØ
            
        Returns:
            ÊâÄÊúâÊ®°ÂûãÁöÑÊÄßËÉΩÁªìÊûú
        """
        if not self.models:
            raise ValueError("No models registered for competition")
        
        if any(data is None for data in [self.train_data, self.test_data, 
                                        self.train_labels, self.test_labels]):
            raise ValueError("Data not setup. Call setup_data() first")
        
        print("\nüèÜ Starting Model Competition...")
        print("=" * 50)
        
        for model_name, model in self.models.items():
            if verbose:
                print(f"\nüîß Evaluating model: {model_name}")
            
            try:
                # ËØÑ‰º∞Âçï‰∏™Ê®°Âûã
                performance = self._evaluate_single_model(
                    model_name, model, verbose
                )
                self.results[model_name] = performance
                
                if verbose:
                    print(f"‚úÖ {model_name} evaluation completed")
                    self._print_model_summary(performance)
                    
            except Exception as e:
                print(f"‚ùå Error evaluating {model_name}: {e}")
                if verbose:
                    import traceback
                    traceback.print_exc()
        
        # ÁîüÊàêÁ´ûËµõÊä•Âëä
        if verbose:
            self._print_competition_report()
        
        return self.results
    
    def _evaluate_single_model(self, name: str, model: UnsupervisedDetector,
                              verbose: bool = True) -> ModelPerformance:
        """ËØÑ‰º∞Âçï‰∏™Ê®°Âûã"""
        import time
        
        # 1. ËÆ≠ÁªÉÊ®°Âûã
        start_time = time.time()
        model.fit(self.train_data, self.train_labels)
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   -> Training completed in {training_time:.2f}s")
        
        # 2. Ê£ÄÊµãÂºÇÂ∏∏
        start_time = time.time()
        detection_timestamps, anomaly_scores = model.detect_anomalies(self.test_data)
        detection_time = time.time() - start_time
        
        if verbose:
            print(f"   -> Detection completed in {detection_time:.2f}s")
            print(f"   -> Detected {len(detection_timestamps)} anomalies")
        
        # 3. ËÆ°ÁÆóÊÄßËÉΩÊåáÊ†á
        performance = self._compute_performance_metrics(
            name, detection_timestamps, anomaly_scores,
            training_time, detection_time, model.get_model_summary()
        )
        
        return performance
    
    def _compute_performance_metrics(self, model_name: str,
                                   detection_timestamps: List[datetime],
                                   anomaly_scores: np.ndarray,
                                   training_time: float,
                                   detection_time: float,
                                   model_params: Dict) -> ModelPerformance:
        """ËÆ°ÁÆóËØ¶ÁªÜÁöÑÊÄßËÉΩÊåáÊ†á"""
        
        # 1. ÂàõÂª∫‰∫åËøõÂà∂È¢ÑÊµãÂêëÈáè
        predictions = self._create_prediction_vector(detection_timestamps)
        
        # 2. ÂØπÈΩêÈ¢ÑÊµãÂíåÁúüÂÆûÊ†áÁ≠æ
        aligned_predictions, aligned_labels = self._align_predictions_labels(predictions)
        
        # 3. ËÆ°ÁÆóÂü∫Á°ÄÊåáÊ†á
        tp = np.sum((aligned_predictions == 1) & (aligned_labels == 1))
        fp = np.sum((aligned_predictions == 1) & (aligned_labels == 0))
        fn = np.sum((aligned_predictions == 0) & (aligned_labels == 1))
        tn = np.sum((aligned_predictions == 0) & (aligned_labels == 0))
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # 4. ËÆ°ÁÆóROCÂíåPRÊõ≤Á∫ø
        try:
            # ÂØπÂºÇÂ∏∏ÂàÜÊï∞ËøõË°åÂØπÈΩê
            aligned_scores = self._align_scores(anomaly_scores)
            
            if len(np.unique(aligned_labels)) > 1:  # Á°Æ‰øùÊúâÊ≠£Ë¥üÊ†∑Êú¨
                auc_roc = roc_auc_score(aligned_labels, aligned_scores)
                
                # ËÆ°ÁÆóPR AUC
                precision_curve, recall_curve, _ = precision_recall_curve(aligned_labels, aligned_scores)
                auc_pr = auc(recall_curve, precision_curve)
            else:
                auc_roc = 0.0
                auc_pr = 0.0
                
        except Exception as e:
            print(f"Warning: Could not compute AUC metrics for {model_name}: {e}")
            auc_roc = 0.0
            auc_pr = 0.0
        
        return ModelPerformance(
            model_name=model_name,
            precision=precision,
            recall=recall,
            f1_score=f1,
            auc_pr=auc_pr,
            auc_roc=auc_roc,
            true_positives=tp,
            false_positives=fp,
            false_negatives=fn,
            true_negatives=tn,
            total_detections=len(detection_timestamps),
            total_true_events=len(self.ground_truth_events),
            anomaly_scores=anomaly_scores,
            detection_timestamps=detection_timestamps,
            training_time=training_time,
            detection_time=detection_time,
            model_params=model_params
        )
    
    def _create_prediction_vector(self, detection_timestamps: List[datetime]) -> pd.Series:
        """ÂàõÂª∫‰∫åËøõÂà∂È¢ÑÊµãÂêëÈáè"""
        predictions = pd.Series(0, index=self.test_data.index)
        
        for det_time in detection_timestamps:
            # Âú®ÂÆπÂøçÁ™óÂè£ÂÜÖÊ†áËÆ∞‰∏∫Ê£ÄÊµãÂà∞
            window_start = det_time - self.tolerance_window
            window_end = det_time + self.tolerance_window
            
            mask = (predictions.index >= window_start) & (predictions.index <= window_end)
            predictions.loc[mask] = 1
        
        return predictions
    
    def _align_predictions_labels(self, predictions: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """ÂØπÈΩêÈ¢ÑÊµãÂíåÊ†áÁ≠æ"""
        # Á°Æ‰øùÁ¥¢ÂºïÂØπÈΩê
        common_index = predictions.index.intersection(self.test_labels.index)
        
        aligned_predictions = predictions.reindex(common_index, fill_value=0).values
        aligned_labels = self.test_labels.reindex(common_index, fill_value=0).values
        
        return aligned_predictions, aligned_labels
    
    def _align_scores(self, anomaly_scores: np.ndarray) -> np.ndarray:
        """ÂØπÈΩêÂºÇÂ∏∏ÂàÜÊï∞"""
        # Áî±‰∫éÂºÇÂ∏∏ÂàÜÊï∞ÂèØËÉΩÊØîtest_dataÁü≠ÔºàÂ∫èÂàóÂ§ÑÁêÜÔºâÔºåÈúÄË¶ÅÂ§ÑÁêÜÂØπÈΩê
        if len(anomaly_scores) == len(self.test_data):
            return anomaly_scores
        elif len(anomaly_scores) < len(self.test_data):
            # Áî®ÊúÄÂêé‰∏Ä‰∏™ÂÄºÂ°´ÂÖÖ
            aligned_scores = np.full(len(self.test_data), anomaly_scores[-1])
            aligned_scores[:len(anomaly_scores)] = anomaly_scores
            return aligned_scores
        else:
            # Êà™Êñ≠
            return anomaly_scores[:len(self.test_data)]
    
    def _print_model_summary(self, performance: ModelPerformance) -> None:
        """ÊâìÂç∞Âçï‰∏™Ê®°ÂûãÁöÑÊÄßËÉΩÊëòË¶Å"""
        print(f"   üìä Performance Summary:")
        print(f"      Precision: {performance.precision:.3f}")
        print(f"      Recall: {performance.recall:.3f}")
        print(f"      F1-Score: {performance.f1_score:.3f}")
        print(f"      AUC-PR: {performance.auc_pr:.3f}")
        print(f"      AUC-ROC: {performance.auc_roc:.3f}")
        print(f"      Detections: {performance.total_detections}")
        print(f"      True Events: {performance.total_true_events}")
    
    def _print_competition_report(self) -> None:
        """ÊâìÂç∞Á´ûËµõÊÄªÁªìÊä•Âëä"""
        print("\n" + "=" * 60)
        print("üèÜ MODEL COMPETITION RESULTS")
        print("=" * 60)
        
        if not self.results:
            print("No results available")
            return
        
        # ÊåâF1ÂàÜÊï∞ÊéíÂ∫è
        sorted_results = sorted(
            self.results.values(),
            key=lambda x: x.f1_score,
            reverse=True
        )
        
        print(f"\nüìä Performance Ranking (by F1-Score):")
        print("-" * 80)
        print(f"{'Rank':<4} {'Model':<20} {'F1':<6} {'Precision':<9} {'Recall':<7} {'AUC-PR':<7} {'Detections':<11}")
        print("-" * 80)
        
        for i, result in enumerate(sorted_results, 1):
            print(f"{i:<4} {result.model_name:<20} {result.f1_score:.3f}  "
                  f"{result.precision:.3f}     {result.recall:.3f}   "
                  f"{result.auc_pr:.3f}   {result.total_detections:<11}")
        
        # Êé®ËçêÊúÄ‰Ω≥Ê®°Âûã
        if sorted_results:
            best_model = sorted_results[0]
            print(f"\nü•á RECOMMENDED MODEL: {best_model.model_name}")
            print(f"   F1-Score: {best_model.f1_score:.3f}")
            print(f"   Balanced performance with {best_model.true_positives} true positives")
            print(f"   and {best_model.false_positives} false positives")
    
    def get_winner(self) -> Optional[str]:
        """Ëé∑ÂèñËé∑ËÉúÊ®°ÂûãÂêçÁß∞ÔºàÂü∫‰∫éF1ÂàÜÊï∞Ôºâ"""
        if not self.results:
            return None
        
        best_model = max(self.results.values(), key=lambda x: x.f1_score)
        return best_model.model_name
    
    def get_detailed_report(self) -> pd.DataFrame:
        """Ëé∑ÂèñËØ¶ÁªÜÁöÑÊØîËæÉÊä•Âëä"""
        if not self.results:
            return pd.DataFrame()
        
        report_data = []
        for result in self.results.values():
            report_data.append({
                'Model': result.model_name,
                'Precision': result.precision,
                'Recall': result.recall,
                'F1-Score': result.f1_score,
                'AUC-PR': result.auc_pr,
                'AUC-ROC': result.auc_roc,
                'True Positives': result.true_positives,
                'False Positives': result.false_positives,
                'False Negatives': result.false_negatives,
                'Total Detections': result.total_detections,
                'Training Time (s)': result.training_time,
                'Detection Time (s)': result.detection_time
            })
        
        return pd.DataFrame(report_data).sort_values('F1-Score', ascending=False)
    
    def save_results(self, filepath: str) -> None:
        """‰øùÂ≠òÁ´ûËµõÁªìÊûú"""
        report_df = self.get_detailed_report()
        report_df.to_csv(filepath, index=False)
        print(f"‚úÖ Results saved to: {filepath}")


# ÂèØËßÜÂåñÂ∑•ÂÖ∑
class BakeoffVisualizer:
    """Á´ûËµõÁªìÊûúÂèØËßÜÂåñÂ∑•ÂÖ∑"""
    
    @staticmethod
    def plot_performance_comparison(bakeoff: ModelBakeoff):
        """ÁªòÂà∂ÊÄßËÉΩÊØîËæÉÂõæ"""
        try:
            import matplotlib.pyplot as plt
            
            if not bakeoff.results:
                print("No results to plot")
                return
            
            models = list(bakeoff.results.keys())
            metrics = ['precision', 'recall', 'f1_score', 'auc_pr']
            metric_names = ['Precision', 'Recall', 'F1-Score', 'AUC-PR']
            
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            axes = axes.flatten()
            
            for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
                values = [getattr(bakeoff.results[model], metric) for model in models]
                
                bars = axes[i].bar(models, values, alpha=0.7)
                axes[i].set_title(f'{metric_name} Comparison')
                axes[i].set_ylabel(metric_name)
                axes[i].set_ylim(0, 1)
                
                # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
                for bar, value in zip(bars, values):
                    axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                                f'{value:.3f}', ha='center', va='bottom')
                
                # ÊóãËΩ¨xËΩ¥Ê†áÁ≠æ
                axes[i].tick_params(axis='x', rotation=45)
                axes[i].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.suptitle('Model Performance Comparison', y=1.02, fontsize=16)
            plt.show()
            
        except ImportError:
            print("Matplotlib not available for plotting")
    
    @staticmethod
    def plot_detection_timeline(bakeoff: ModelBakeoff, model_name: str):
        """ÁªòÂà∂ÁâπÂÆöÊ®°ÂûãÁöÑÊ£ÄÊµãÊó∂Èó¥Á∫ø"""
        try:
            import matplotlib.pyplot as plt
            
            if model_name not in bakeoff.results:
                print(f"Model {model_name} not found in results")
                return
            
            result = bakeoff.results[model_name]
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)
            
            # ‰∏äÂõæÔºöÊï∞ÊçÆÂíåÊ£ÄÊµãÁªìÊûú
            test_data = bakeoff.test_data
            if 'mean_motion' in test_data.columns:
                ax1.plot(test_data.index, test_data['mean_motion'], 'b-', 
                        linewidth=1, alpha=0.7, label='Mean Motion')
            
            # Ê†áËÆ∞ÁúüÂÆûÊú∫Âä®
            for event_time in bakeoff.ground_truth_events:
                ax1.axvline(x=event_time, color='green', alpha=0.8, 
                           linestyle='--', linewidth=2)
            
            # Ê†áËÆ∞Ê£ÄÊµãÂà∞ÁöÑÂºÇÂ∏∏
            for det_time in result.detection_timestamps:
                ax1.axvline(x=det_time, color='red', alpha=0.6, 
                           linestyle='-', linewidth=1)
            
            ax1.set_title(f'{model_name} - Detection Timeline')
            ax1.set_ylabel('Mean Motion')
            ax1.legend(['Data', 'True Maneuvers', 'Detections'])
            ax1.grid(True, alpha=0.3)
            
            # ‰∏ãÂõæÔºöÂºÇÂ∏∏ÂàÜÊï∞
            if len(result.anomaly_scores) > 0:
                score_times = test_data.index[:len(result.anomaly_scores)]
                ax2.plot(score_times, result.anomaly_scores, 'k-', 
                        linewidth=1, alpha=0.7, label='Anomaly Score')
                ax2.set_ylabel('Anomaly Score')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
        except ImportError:
            print("Matplotlib not available for plotting")


# ‰ΩøÁî®Á§∫‰æãÂáΩÊï∞
def run_example_bakeoff():
    """ËøêË°åÁ§∫‰æãÁ´ûËµõ"""
    print("üß™ Running Example Model Bakeoff...")
    
    # ÁîüÊàêÊ®°ÊãüÊï∞ÊçÆ
    dates = pd.date_range('2020-01-01', '2020-12-31', freq='1D')
    n_samples = len(dates)
    
    # Ê≠£Â∏∏Êï∞ÊçÆ + Êú∫Âä®‰∫ã‰ª∂
    data = pd.DataFrame({
        'mean_motion': np.random.normal(1.0027, 0.00005, n_samples),
        'eccentricity': np.random.normal(0.001, 0.0002, n_samples),
        'inclination': np.random.normal(0.1, 0.01, n_samples)
    }, index=dates)
    
    # Ê∑ªÂä†Êú∫Âä®
    maneuver_indices = [50, 150, 250, 300]
    labels = pd.Series(0, index=dates)
    for idx in maneuver_indices:
        if idx < len(data):
            data.iloc[idx]['mean_motion'] += np.random.normal(0, 0.001)
            labels.iloc[idx] = 1
    
    # ÂàÜÂâ≤Êï∞ÊçÆ
    split_idx = len(data) // 2
    train_data, test_data = data.iloc[:split_idx], data.iloc[split_idx:]
    train_labels, test_labels = labels.iloc[:split_idx], labels.iloc[split_idx:]
    
    # ÂàõÂª∫Á´ûËµõÊ°ÜÊû∂
    bakeoff = ModelBakeoff()
    
    # Ê≥®ÂÜåÊ®°ÂûãÔºàËøôÈáåÁî®ÁÆÄÂåñÁâàÊú¨ËøõË°åÊºîÁ§∫Ôºâ
    # ÂÆûÈôÖ‰ΩøÁî®Êó∂Â∫îËØ•Ê≥®ÂÜåÁúüÂÆûÁöÑÊ£ÄÊµãÂô®
    
    # ËÆæÁΩÆÊï∞ÊçÆ
    bakeoff.setup_data(train_data, test_data, train_labels, test_labels)
    
    print("Example bakeoff setup completed. Ready for real models!")
    
    return bakeoff

if __name__ == "__main__":
    run_example_bakeoff()