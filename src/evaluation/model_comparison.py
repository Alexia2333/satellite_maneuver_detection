# src/evaluation/model_comparison.py
"""
æ— ç›‘ç£æ¨¡å‹ç«èµ›æ¡†æ¶ (Model Bakeoff Framework)

è¿™ä¸ªæ¨¡å—å®ç°äº†æ–¹æ¡ˆä¸­æ¨èçš„"æ¨¡å‹ç«èµ›"æ¦‚å¿µï¼Œç”¨äºå®¢è§‚åœ°è¯„ä¼°å’Œæ¯”è¾ƒ
ä¸åŒæ— ç›‘ç£æœºåŠ¨æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨2020-2022å¹´çš„é£äº‘4AæœºåŠ¨æ—¥å¿—
ä½œä¸ºåœ°é¢çœŸå®å€¼ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å„ç§æ— ç›‘ç£ç®—æ³•è¿›è¡Œä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ã€‚

Key Features:
- ç»Ÿä¸€çš„æ¨¡å‹æ¥å£å’Œè¯„ä¼°æ¡†æ¶
- æ ‡å‡†åŒ–çš„æ€§èƒ½æŒ‡æ ‡è®¡ç®—
- è‡ªåŠ¨åŒ–çš„æ¨¡å‹é€‰æ‹©å’Œæ’å
- è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Š
- å¯è§†åŒ–çš„ç»“æœå¯¹æ¯”
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Protocol
from datetime import datetime, timedelta
from dataclasses import dataclass
from abc import ABC, abstractmethod

import warnings
from sklearn.metrics import (
    precision_score, recall_score, f1_score, roc_auc_score,
    precision_recall_curve, roc_curve, auc, confusion_matrix
)

# å¯¼å…¥æˆ‘ä»¬çš„æ£€æµ‹å™¨
from src.models.unsupervised.particle_filter import ParticleFilterDetector
from src.models.unsupervised.lstm_vae import LSTMVAEDetector

@dataclass
class ModelPerformance:
    """æ¨¡å‹æ€§èƒ½ç»“æœæ•°æ®ç±»"""
    model_name: str
    precision: float
    recall: float
    f1_score: float
    auc_pr: float  # PRæ›²çº¿ä¸‹é¢ç§¯
    auc_roc: float  # ROCæ›²çº¿ä¸‹é¢ç§¯
    true_positives: int
    false_positives: int
    false_negatives: int
    true_negatives: int
    total_detections: int
    total_true_events: int
    anomaly_scores: np.ndarray
    detection_timestamps: List[datetime]
    training_time: float
    detection_time: float
    model_params: Dict[str, Any]

class UnsupervisedDetector(Protocol):
    """æ— ç›‘ç£æ£€æµ‹å™¨çš„ç»Ÿä¸€æ¥å£åè®®"""
    
    def fit(self, train_data: pd.DataFrame, 
            maneuver_labels: Optional[pd.Series] = None) -> None:
        """è®­ç»ƒæ£€æµ‹å™¨"""
        ...
    
    def detect_anomalies(self, test_data: pd.DataFrame) -> Tuple[List[datetime], np.ndarray]:
        """æ£€æµ‹å¼‚å¸¸ï¼Œè¿”å›å¼‚å¸¸æ—¶åˆ»å’Œå¼‚å¸¸åˆ†æ•°"""
        ...
    
    def get_model_summary(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ‘˜è¦"""
        ...

class ModelBakeoff:
    """
    æ— ç›‘ç£æ¨¡å‹ç«èµ›æ¡†æ¶
    
    è¿™ä¸ªç±»ç®¡ç†å¤šä¸ªæ— ç›‘ç£æ£€æµ‹å™¨çš„è®­ç»ƒã€è¯„ä¼°å’Œæ¯”è¾ƒï¼Œ
    æä¾›å®¢è§‚çš„æ€§èƒ½è¯„ä¼°å’Œæ¨¡å‹é€‰æ‹©å»ºè®®ã€‚
    """
    
    def __init__(self, 
                 tolerance_window: timedelta = timedelta(days=1),
                 min_time_between_events: timedelta = timedelta(hours=12)):
        """
        åˆå§‹åŒ–ç«èµ›æ¡†æ¶
        
        Args:
            tolerance_window: æ£€æµ‹å®¹å¿çª—å£ï¼ˆæ£€æµ‹åœ¨çœŸå®äº‹ä»¶å‰åæ­¤æ—¶é—´å†…ç®—æ­£ç¡®ï¼‰
            min_time_between_events: è¿ç»­äº‹ä»¶é—´æœ€å°æ—¶é—´é—´éš”
        """
        self.tolerance_window = tolerance_window
        self.min_time_between_events = min_time_between_events
        
        # æ³¨å†Œçš„æ¨¡å‹
        self.models: Dict[str, UnsupervisedDetector] = {}
        self.model_configs: Dict[str, Dict] = {}
        
        # è¯„ä¼°ç»“æœ
        self.results: Dict[str, ModelPerformance] = {}
        self.ground_truth_events: List[datetime] = []
        
        # æ•°æ®é›†ä¿¡æ¯
        self.train_data: Optional[pd.DataFrame] = None
        self.test_data: Optional[pd.DataFrame] = None
        self.train_labels: Optional[pd.Series] = None
        self.test_labels: Optional[pd.Series] = None
        
        print("ğŸ Model Bakeoff Framework initialized")
    
    def register_model(self, name: str, model: UnsupervisedDetector, 
                      config: Optional[Dict] = None) -> None:
        """
        æ³¨å†Œå‚èµ›æ¨¡å‹
        
        Args:
            name: æ¨¡å‹åç§°
            model: æ£€æµ‹å™¨å®ä¾‹
            config: æ¨¡å‹é…ç½®ä¿¡æ¯
        """
        self.models[name] = model
        self.model_configs[name] = config or {}
        print(f"âœ… Registered model: {name}")
    
    def setup_data(self, train_data: pd.DataFrame, test_data: pd.DataFrame,
                   train_labels: pd.Series, test_labels: pd.Series) -> None:
        """
        è®¾ç½®è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        
        Args:
            train_data: è®­ç»ƒæ•°æ®
            test_data: æµ‹è¯•æ•°æ®  
            train_labels: è®­ç»ƒæ ‡ç­¾
            test_labels: æµ‹è¯•æ ‡ç­¾
        """
        self.train_data = train_data
        self.test_data = test_data
        self.train_labels = train_labels
        self.test_labels = test_labels
        
        # æå–çœŸå®äº‹ä»¶æ—¶åˆ»
        self.ground_truth_events = test_labels[test_labels == 1].index.tolist()
        
        print(f"ğŸ“Š Data setup complete:")
        print(f"   -> Training data: {len(train_data)} records")
        print(f"   -> Test data: {len(test_data)} records")
        print(f"   -> Ground truth events: {len(self.ground_truth_events)}")
    
    def run_competition(self, verbose: bool = True) -> Dict[str, ModelPerformance]:
        """
        è¿è¡Œæ¨¡å‹ç«èµ›
        
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½ç»“æœ
        """
        if not self.models:
            raise ValueError("No models registered for competition")
        
        if any(data is None for data in [self.train_data, self.test_data, 
                                        self.train_labels, self.test_labels]):
            raise ValueError("Data not setup. Call setup_data() first")
        
        print("\nğŸ† Starting Model Competition...")
        print("=" * 50)
        
        for model_name, model in self.models.items():
            if verbose:
                print(f"\nğŸ”§ Evaluating model: {model_name}")
            
            try:
                # è¯„ä¼°å•ä¸ªæ¨¡å‹
                performance = self._evaluate_single_model(
                    model_name, model, verbose
                )
                self.results[model_name] = performance
                
                if verbose:
                    print(f"âœ… {model_name} evaluation completed")
                    self._print_model_summary(performance)
                    
            except Exception as e:
                print(f"âŒ Error evaluating {model_name}: {e}")
                if verbose:
                    import traceback
                    traceback.print_exc()
        
        # ç”Ÿæˆç«èµ›æŠ¥å‘Š
        if verbose:
            self._print_competition_report()
        
        return self.results
    
    def _evaluate_single_model(self, name: str, model: UnsupervisedDetector,
                              verbose: bool = True) -> ModelPerformance:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        import time
        
        # 1. è®­ç»ƒæ¨¡å‹
        start_time = time.time()
        model.fit(self.train_data, self.train_labels)
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   -> Training completed in {training_time:.2f}s")
        
        # 2. æ£€æµ‹å¼‚å¸¸
        start_time = time.time()
        detection_timestamps, anomaly_scores = model.detect_anomalies(self.test_data)
        detection_time = time.time() - start_time
        
        if verbose:
            print(f"   -> Detection completed in {detection_time:.2f}s")
            print(f"   -> Detected {len(detection_timestamps)} anomalies")
        
        # 3. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance = self._compute_performance_metrics(
            name, detection_timestamps, anomaly_scores,
            training_time, detection_time, model.get_model_summary()
        )
        
        return performance
    
    def _compute_performance_metrics(self, model_name: str,
                                   detection_timestamps: List[datetime],
                                   anomaly_scores: np.ndarray,
                                   training_time: float,
                                   detection_time: float,
                                   model_params: Dict) -> ModelPerformance:
        """è®¡ç®—è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡"""
        
        # 1. åˆ›å»ºäºŒè¿›åˆ¶é¢„æµ‹å‘é‡
        predictions = self._create_prediction_vector(detection_timestamps)
        
        # 2. å¯¹é½é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
        aligned_predictions, aligned_labels = self._align_predictions_labels(predictions)
        
        # 3. è®¡ç®—åŸºç¡€æŒ‡æ ‡
        tp = np.sum((aligned_predictions == 1) & (aligned_labels == 1))
        fp = np.sum((aligned_predictions == 1) & (aligned_labels == 0))
        fn = np.sum((aligned_predictions == 0) & (aligned_labels == 1))
        tn = np.sum((aligned_predictions == 0) & (aligned_labels == 0))
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # 4. è®¡ç®—ROCå’ŒPRæ›²çº¿
        try:
            # å¯¹å¼‚å¸¸åˆ†æ•°è¿›è¡Œå¯¹é½
            aligned_scores = self._align_scores(anomaly_scores)
            
            if len(np.unique(aligned_labels)) > 1:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
                auc_roc = roc_auc_score(aligned_labels, aligned_scores)
                
                # è®¡ç®—PR AUC
                precision_curve, recall_curve, _ = precision_recall_curve(aligned_labels, aligned_scores)
                auc_pr = auc(recall_curve, precision_curve)
            else:
                auc_roc = 0.0
                auc_pr = 0.0
                
        except Exception as e:
            print(f"Warning: Could not compute AUC metrics for {model_name}: {e}")
            auc_roc = 0.0
            auc_pr = 0.0
        
        return ModelPerformance(
            model_name=model_name,
            precision=precision,
            recall=recall,
            f1_score=f1,
            auc_pr=auc_pr,
            auc_roc=auc_roc,
            true_positives=tp,
            false_positives=fp,
            false_negatives=fn,
            true_negatives=tn,
            total_detections=len(detection_timestamps),
            total_true_events=len(self.ground_truth_events),
            anomaly_scores=anomaly_scores,
            detection_timestamps=detection_timestamps,
            training_time=training_time,
            detection_time=detection_time,
            model_params=model_params
        )
    
    def _create_prediction_vector(self, detection_timestamps: List[datetime]) -> pd.Series:
        """åˆ›å»ºäºŒè¿›åˆ¶é¢„æµ‹å‘é‡"""
        predictions = pd.Series(0, index=self.test_data.index)
        
        for det_time in detection_timestamps:
            # åœ¨å®¹å¿çª—å£å†…æ ‡è®°ä¸ºæ£€æµ‹åˆ°
            window_start = det_time - self.tolerance_window
            window_end = det_time + self.tolerance_window
            
            mask = (predictions.index >= window_start) & (predictions.index <= window_end)
            predictions.loc[mask] = 1
        
        return predictions
    
    def _align_predictions_labels(self, predictions: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """å¯¹é½é¢„æµ‹å’Œæ ‡ç­¾"""
        # ç¡®ä¿ç´¢å¼•å¯¹é½
        common_index = predictions.index.intersection(self.test_labels.index)
        
        aligned_predictions = predictions.reindex(common_index, fill_value=0).values
        aligned_labels = self.test_labels.reindex(common_index, fill_value=0).values
        
        return aligned_predictions, aligned_labels
    
    def _align_scores(self, anomaly_scores: np.ndarray) -> np.ndarray:
        """å¯¹é½å¼‚å¸¸åˆ†æ•°"""
        # ç”±äºå¼‚å¸¸åˆ†æ•°å¯èƒ½æ¯”test_dataçŸ­ï¼ˆåºåˆ—å¤„ç†ï¼‰ï¼Œéœ€è¦å¤„ç†å¯¹é½
        if len(anomaly_scores) == len(self.test_data):
            return anomaly_scores
        elif len(anomaly_scores) < len(self.test_data):
            # ç”¨æœ€åä¸€ä¸ªå€¼å¡«å……
            aligned_scores = np.full(len(self.test_data), anomaly_scores[-1])
            aligned_scores[:len(anomaly_scores)] = anomaly_scores
            return aligned_scores
        else:
            # æˆªæ–­
            return anomaly_scores[:len(self.test_data)]
    
    def _print_model_summary(self, performance: ModelPerformance) -> None:
        """æ‰“å°å•ä¸ªæ¨¡å‹çš„æ€§èƒ½æ‘˜è¦"""
        print(f"   ğŸ“Š Performance Summary:")
        print(f"      Precision: {performance.precision:.3f}")
        print(f"      Recall: {performance.recall:.3f}")
        print(f"      F1-Score: {performance.f1_score:.3f}")
        print(f"      AUC-PR: {performance.auc_pr:.3f}")
        print(f"      AUC-ROC: {performance.auc_roc:.3f}")
        print(f"      Detections: {performance.total_detections}")
        print(f"      True Events: {performance.total_true_events}")
    
    def _print_competition_report(self) -> None:
        """æ‰“å°ç«èµ›æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ† MODEL COMPETITION RESULTS")
        print("=" * 60)
        
        if not self.results:
            print("No results available")
            return
        
        # æŒ‰F1åˆ†æ•°æ’åº
        sorted_results = sorted(
            self.results.values(),
            key=lambda x: x.f1_score,
            reverse=True
        )
        
        print(f"\nğŸ“Š Performance Ranking (by F1-Score):")
        print("-" * 80)
        print(f"{'Rank':<4} {'Model':<20} {'F1':<6} {'Precision':<9} {'Recall':<7} {'AUC-PR':<7} {'Detections':<11}")
        print("-" * 80)
        
        for i, result in enumerate(sorted_results, 1):
            print(f"{i:<4} {result.model_name:<20} {result.f1_score:.3f}  "
                  f"{result.precision:.3f}     {result.recall:.3f}   "
                  f"{result.auc_pr:.3f}   {result.total_detections:<11}")
        
        # æ¨èæœ€ä½³æ¨¡å‹
        if sorted_results:
            best_model = sorted_results[0]
            print(f"\nğŸ¥‡ RECOMMENDED MODEL: {best_model.model_name}")
            print(f"   F1-Score: {best_model.f1_score:.3f}")
            print(f"   Balanced performance with {best_model.true_positives} true positives")
            print(f"   and {best_model.false_positives} false positives")
    
    def get_winner(self) -> Optional[str]:
        """è·å–è·èƒœæ¨¡å‹åç§°ï¼ˆåŸºäºF1åˆ†æ•°ï¼‰"""
        if not self.results:
            return None
        
        best_model = max(self.results.values(), key=lambda x: x.f1_score)
        return best_model.model_name
    
    def get_detailed_report(self) -> pd.DataFrame:
        """è·å–è¯¦ç»†çš„æ¯”è¾ƒæŠ¥å‘Š"""
        if not self.results:
            return pd.DataFrame()
        
        report_data = []
        for result in self.results.values():
            report_data.append({
                'Model': result.model_name,
                'Precision': result.precision,
                'Recall': result.recall,
                'F1-Score': result.f1_score,
                'AUC-PR': result.auc_pr,
                'AUC-ROC': result.auc_roc,
                'True Positives': result.true_positives,
                'False Positives': result.false_positives,
                'False Negatives': result.false_negatives,
                'Total Detections': result.total_detections,
                'Training Time (s)': result.training_time,
                'Detection Time (s)': result.detection_time
            })
        
        return pd.DataFrame(report_data).sort_values('F1-Score', ascending=False)
    
    def save_results(self, filepath: str) -> None:
        """ä¿å­˜ç«èµ›ç»“æœ"""
        report_df = self.get_detailed_report()
        report_df.to_csv(filepath, index=False)
        print(f"âœ… Results saved to: {filepath}")


# å¯è§†åŒ–å·¥å…·
class BakeoffVisualizer:
    """ç«èµ›ç»“æœå¯è§†åŒ–å·¥å…·"""
    
    @staticmethod
    def plot_performance_comparison(bakeoff: ModelBakeoff):
        """ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾"""
        try:
            import matplotlib.pyplot as plt
            
            if not bakeoff.results:
                print("No results to plot")
                return
            
            models = list(bakeoff.results.keys())
            metrics = ['precision', 'recall', 'f1_score', 'auc_pr']
            metric_names = ['Precision', 'Recall', 'F1-Score', 'AUC-PR']
            
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            axes = axes.flatten()
            
            for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
                values = [getattr(bakeoff.results[model], metric) for model in models]
                
                bars = axes[i].bar(models, values, alpha=0.7)
                axes[i].set_title(f'{metric_name} Comparison')
                axes[i].set_ylabel(metric_name)
                axes[i].set_ylim(0, 1)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, value in zip(bars, values):
                    axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                                f'{value:.3f}', ha='center', va='bottom')
                
                # æ—‹è½¬xè½´æ ‡ç­¾
                axes[i].tick_params(axis='x', rotation=45)
                axes[i].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.suptitle('Model Performance Comparison', y=1.02, fontsize=16)
            plt.show()
            
        except ImportError:
            print("Matplotlib not available for plotting")
    
    @staticmethod
    def plot_detection_timeline(bakeoff: ModelBakeoff, model_name: str):
        """ç»˜åˆ¶ç‰¹å®šæ¨¡å‹çš„æ£€æµ‹æ—¶é—´çº¿"""
        try:
            import matplotlib.pyplot as plt
            
            if model_name not in bakeoff.results:
                print(f"Model {model_name} not found in results")
                return
            
            result = bakeoff.results[model_name]
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)
            
            # ä¸Šå›¾ï¼šæ•°æ®å’Œæ£€æµ‹ç»“æœ
            test_data = bakeoff.test_data
            if 'mean_motion' in test_data.columns:
                ax1.plot(test_data.index, test_data['mean_motion'], 'b-', 
                        linewidth=1, alpha=0.7, label='Mean Motion')
            
            # æ ‡è®°çœŸå®æœºåŠ¨
            for event_time in bakeoff.ground_truth_events:
                ax1.axvline(x=event_time, color='green', alpha=0.8, 
                           linestyle='--', linewidth=2)
            
            # æ ‡è®°æ£€æµ‹åˆ°çš„å¼‚å¸¸
            for det_time in result.detection_timestamps:
                ax1.axvline(x=det_time, color='red', alpha=0.6, 
                           linestyle='-', linewidth=1)
            
            ax1.set_title(f'{model_name} - Detection Timeline')
            ax1.set_ylabel('Mean Motion')
            ax1.legend(['Data', 'True Maneuvers', 'Detections'])
            ax1.grid(True, alpha=0.3)
            
            # ä¸‹å›¾ï¼šå¼‚å¸¸åˆ†æ•°
            if len(result.anomaly_scores) > 0:
                score_times = test_data.index[:len(result.anomaly_scores)]
                ax2.plot(score_times, result.anomaly_scores, 'k-', 
                        linewidth=1, alpha=0.7, label='Anomaly Score')
                ax2.set_ylabel('Anomaly Score')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
        except ImportError:
            print("Matplotlib not available for plotting")


# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def run_example_bakeoff():
    """è¿è¡Œç¤ºä¾‹ç«èµ›"""
    print("ğŸ§ª Running Example Model Bakeoff...")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    dates = pd.date_range('2020-01-01', '2020-12-31', freq='1D')
    n_samples = len(dates)
    
    # æ­£å¸¸æ•°æ® + æœºåŠ¨äº‹ä»¶
    data = pd.DataFrame({
        'mean_motion': np.random.normal(1.0027, 0.00005, n_samples),
        'eccentricity': np.random.normal(0.001, 0.0002, n_samples),
        'inclination': np.random.normal(0.1, 0.01, n_samples)
    }, index=dates)
    
    # æ·»åŠ æœºåŠ¨
    maneuver_indices = [50, 150, 250, 300]
    labels = pd.Series(0, index=dates)
    for idx in maneuver_indices:
        if idx < len(data):
            data.iloc[idx]['mean_motion'] += np.random.normal(0, 0.001)
            labels.iloc[idx] = 1
    
    # åˆ†å‰²æ•°æ®
    split_idx = len(data) // 2
    train_data, test_data = data.iloc[:split_idx], data.iloc[split_idx:]
    train_labels, test_labels = labels.iloc[:split_idx], labels.iloc[split_idx:]
    
    # åˆ›å»ºç«èµ›æ¡†æ¶
    bakeoff = ModelBakeoff()
    
    # æ³¨å†Œæ¨¡å‹ï¼ˆè¿™é‡Œç”¨ç®€åŒ–ç‰ˆæœ¬è¿›è¡Œæ¼”ç¤ºï¼‰
    # å®é™…ä½¿ç”¨æ—¶åº”è¯¥æ³¨å†ŒçœŸå®çš„æ£€æµ‹å™¨
    
    # è®¾ç½®æ•°æ®
    bakeoff.setup_data(train_data, test_data, train_labels, test_labels)
    
    print("Example bakeoff setup completed. Ready for real models!")
    
    return bakeoff

if __name__ == "__main__":
    run_example_bakeoff()