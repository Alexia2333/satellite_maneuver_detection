# scripts/improved_xgboost_task.py
"""
æ”¹è¿›çš„XGBoostæœºåŠ¨é¢„æµ‹æ–¹æ¡ˆ
é’ˆå¯¹é£äº‘4Aå«æ˜Ÿè½¨é“ç»´æŒæœºåŠ¨çš„ç‰¹å¾å·¥ç¨‹å’Œæ£€æµ‹ä¼˜åŒ–

æ”¹è¿›è¦ç‚¹ï¼š
1. ç‰¹å¾å·¥ç¨‹ï¼šé‡åŒ–æŒç»­æ¼‚ç§»å’Œç´¯ç§¯æ¼‚ç§»å‹åŠ›
2. å­¦ä¹ ç›®æ ‡ï¼šé¢„æµ‹æœºåŠ¨å¼•èµ·çš„çŠ¶æ€è·³å˜å¤§å°
3. é˜ˆå€¼ä¼˜åŒ–ï¼šåŸºäºæ¼‚ç§»ç´¯ç§¯æ¨¡å¼åŠ¨æ€è°ƒæ•´
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

print("ğŸš€ æ”¹è¿›çš„XGBoostæœºåŠ¨é¢„æµ‹æ–¹æ¡ˆ")
print("=" * 50)

# =====================================================================
# æ”¹è¿›çš„ç‰¹å¾å·¥ç¨‹
# =====================================================================
def create_drift_features(tle_data, target_col='mean_motion'):
    """
    åˆ›å»ºæ¼‚ç§»ç›¸å…³çš„ç‰¹å¾
    
    ç‰¹å¾1ï¼šé‡åŒ–"æŒç»­æ¼‚ç§»" - æè¿°å‚æ•°åç¦»å…¶æ­£å¸¸ä¸­å¿ƒçš„ç¨‹åº¦
    ç‰¹å¾2ï¼šå¼•å…¥"ç´¯ç§¯æ¼‚ç§»å‹åŠ›" - æ¼‚ç§»æ˜¯æŒç»­ç´¯ç§¯çš„
    """
    print("\nğŸ› ï¸ åˆ›å»ºæ¼‚ç§»ç‰¹å¾")
    print("-" * 20)
    
    data = tle_data.set_index('epoch').copy()
    
    # åŸºç¡€è½¨é“å‚æ•°
    orbit_params = ['mean_motion', 'eccentricity', 'inclination']
    available_params = [col for col in orbit_params if col in data.columns]
    
    if not available_params:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°è½¨é“å‚æ•°åˆ—: {orbit_params}")
        return None
    
    print(f"   åŸºç¡€è½¨é“å‚æ•°: {available_params}")
    
    enhanced_data = data[available_params].copy()
    
    # ==== ç‰¹å¾1ï¼šé‡åŒ–æŒç»­æ¼‚ç§» ====
    # è®¡ç®—é•¿æœŸæ»šåŠ¨å‡å€¼ï¼ˆæ­£å¸¸ä¸­å¿ƒï¼‰
    long_windows = [15, 30, 45]  
    
    for col in available_params:
        for window in long_windows:
            # é•¿æœŸå‡å€¼
            col_mean = data[col].rolling(window, min_periods=window//2).mean()
            # åç¦»ç¨‹åº¦
            enhanced_data[f'{col}_drift_from_{window}d_mean'] = data[col] - col_mean
            # ç›¸å¯¹åç¦»ï¼ˆæ ‡å‡†åŒ–ï¼‰
            col_std = data[col].rolling(window, min_periods=window//2).std()
            enhanced_data[f'{col}_drift_zscore_{window}d'] = (data[col] - col_mean) / (col_std + 1e-8)
    
    # æ¼‚ç§»é€Ÿåº¦å’Œæ–¹å‘ï¼ˆè¶‹åŠ¿æ–œç‡ï¼‰
    trend_windows = [7, 14, 21]
    for col in available_params:
        for window in trend_windows:
            # ä½¿ç”¨çº¿æ€§å›å½’è®¡ç®—è¶‹åŠ¿æ–œç‡
            enhanced_data[f'{col}_trend_slope_{window}d'] = data[col].rolling(window).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0
            )
    
    # ==== ç‰¹å¾2ï¼šç´¯ç§¯æ¼‚ç§»å‹åŠ› ====
    # çŸ­æœŸæ¼‚ç§»çš„ç´¯ç§¯å€¼
    short_windows = [3, 7, 14]
    
    for col in available_params:
        # é¦–å…ˆè®¡ç®—çŸ­æœŸæ¼‚ç§»
        short_mean = data[col].rolling(7).mean()
        short_drift = (data[col] - short_mean).abs()
        
        # ç´¯ç§¯æ¼‚ç§»å‹åŠ›ï¼ˆæ»šåŠ¨ç´¯åŠ ï¼‰
        for window in short_windows:
            enhanced_data[f'{col}_cumulative_drift_{window}d'] = short_drift.rolling(window).sum()
            # åŠ æƒç´¯ç§¯ï¼ˆæœ€è¿‘çš„æ¼‚ç§»æƒé‡æ›´å¤§ï¼‰
            weights = np.exp(np.linspace(-1, 0, window))
            enhanced_data[f'{col}_weighted_cumulative_drift_{window}d'] = short_drift.rolling(window).apply(
                lambda x: np.sum(x * weights[-len(x):]) / np.sum(weights[-len(x):])
            )
    
    # ==== ç‰¹å¾3ï¼šå‚æ•°é—´çš„ååŒæ¼‚ç§» ====
    # å¤šä¸ªå‚æ•°åŒæ—¶æ¼‚ç§»å¯èƒ½é¢„ç¤ºç€éœ€è¦æœºåŠ¨
    if len(available_params) > 1:
        # è®¡ç®—æ‰€æœ‰å‚æ•°çš„ç»¼åˆæ¼‚ç§»æŒ‡æ ‡
        for window in [7, 14]:
            drift_cols = [f'{col}_drift_from_{window}d_mean' for col in available_params 
                          if f'{col}_drift_from_{window}d_mean' in enhanced_data.columns]
            if drift_cols:
                # ç»¼åˆæ¼‚ç§»å¹…åº¦ï¼ˆL2èŒƒæ•°ï¼‰
                enhanced_data[f'combined_drift_l2_{window}d'] = np.sqrt(
                    enhanced_data[drift_cols].pow(2).sum(axis=1)
                )
                # ç»¼åˆæ¼‚ç§»æ–¹å‘ä¸€è‡´æ€§
                enhanced_data[f'drift_correlation_{window}d'] = enhanced_data[drift_cols].apply(
                    lambda x: 1 if (x > 0).all() or (x < 0).all() else 0, axis=1
                )
    
    # ==== ç‰¹å¾4ï¼šå†å²æœºåŠ¨æ¨¡å¼ç‰¹å¾ ====
    # è·ç¦»ä¸Šæ¬¡å¤§å¹…å˜åŒ–çš„æ—¶é—´
    for col in available_params:
        # æ£€æµ‹å¤§å¹…å˜åŒ–ï¼ˆå¯èƒ½çš„å†å²æœºåŠ¨ï¼‰
        col_diff = data[col].diff().abs()
        threshold = col_diff.quantile(0.95)
        
        # åˆ›å»ºäº‹ä»¶æ ‡è®°
        events = (col_diff > threshold).astype(int)
        
        # è®¡ç®—è·ç¦»ä¸Šæ¬¡äº‹ä»¶çš„æ—¶é—´
        last_event_idx = pd.Series(range(len(events)), index=events.index)
        last_event_idx[events == 0] = np.nan
        last_event_idx = last_event_idx.fillna(method='ffill')
        
        enhanced_data[f'{col}_days_since_last_jump'] = (
            pd.Series(range(len(events)), index=events.index) - last_event_idx
        )
    
    # ==== ç‰¹å¾5ï¼šç»Ÿè®¡ç‰¹å¾ ====
    # æ·»åŠ åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    stat_windows = [3, 7, 14, 30]
    for col in available_params:
        for window in stat_windows:
            enhanced_data[f'{col}_rolling_mean_{window}d'] = data[col].rolling(window).mean()
            enhanced_data[f'{col}_rolling_std_{window}d'] = data[col].rolling(window).std()
            enhanced_data[f'{col}_rolling_skew_{window}d'] = data[col].rolling(window).skew()
            enhanced_data[f'{col}_rolling_kurt_{window}d'] = data[col].rolling(window).kurt()
    
    # ==== å­¦ä¹ ç›®æ ‡ï¼šé¢„æµ‹æœºåŠ¨å¼•èµ·çš„è·³å˜ ====
    # ç›®æ ‡æ˜¯é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»çš„mean_motionå˜åŒ–é‡ï¼ˆæœºåŠ¨ä¼šå¼•èµ·çªå˜ï¼‰
    enhanced_data['target'] = data[target_col].diff().shift(-1)
    
    # æ·»åŠ ç›®æ ‡çš„ç»å¯¹å€¼ç‰ˆæœ¬ï¼ˆç”¨äºæ£€æµ‹ä»»ä½•æ–¹å‘çš„è·³å˜ï¼‰
    enhanced_data['target_abs'] = enhanced_data['target'].abs()
    
    # åˆ é™¤ç©ºå€¼
    # æ­¥éª¤ 1: å”¯ä¸€éœ€è¦åˆ é™¤çš„æ˜¯æ²¡æœ‰å­¦ä¹ ç›®æ ‡(target)çš„è¡Œ
    enhanced_data = enhanced_data.dropna(subset=['target', 'target_abs'])
    print(f"   è®°å½•æ•° (åœ¨ä¸¢å¼ƒæ— æ•ˆç›®æ ‡å): {len(enhanced_data)}")

    # æ­¥éª¤ 2: å¯¹ç‰¹å¾åˆ—ä¸­çš„NaNå€¼è¿›è¡Œå¡«å……ï¼Œè€Œä¸æ˜¯åˆ é™¤æ•´è¡Œ
    # ä¼˜å…ˆä½¿ç”¨å‰ä¸€ä¸ªæ—¶é—´ç‚¹çš„å€¼å¡«å…… (forward-fill)
    # ç„¶åç”¨åä¸€ä¸ªæ—¶é—´ç‚¹çš„å€¼å¡«å……ï¼Œå¤„ç†æ–‡ä»¶å¼€å¤´çš„NaN
    feature_cols = [col for col in enhanced_data.columns if col not in ['target', 'target_abs']]
    enhanced_data[feature_cols] = enhanced_data[feature_cols].ffill().bfill()

    # æ­¥éª¤ 3: ä½œä¸ºæœ€åçš„ä¿é™©ï¼Œå¦‚æœè¿˜æœ‰NaNï¼Œç”¨æ¯åˆ—çš„ä¸­ä½æ•°å¡«å……
    for col in feature_cols:
        if enhanced_data[col].isnull().any():
            median_val = enhanced_data[col].median()
            if not pd.isna(median_val):
                enhanced_data[col] = enhanced_data[col].fillna(median_val)
            else:
                # å¦‚æœæ•´åˆ—éƒ½æ˜¯NaNï¼Œåˆ™ç”¨0å¡«å……
                enhanced_data[col] = enhanced_data[col].fillna(0)
    
    print(f"âœ… ç‰¹å¾å’ŒNaNå€¼å¤„ç†å®Œæˆ")

    print(f"   è®°å½•æ•° (åœ¨ä¸¢å¼ƒæ— æ•ˆç›®æ ‡å): {len(enhanced_data)}")
    
    print(f"âœ… æ¼‚ç§»ç‰¹å¾åˆ›å»ºå®Œæˆ")
    print(f"   ç‰¹å¾æ•°é‡: {len(enhanced_data.columns)}")
    print(f"   æœ‰æ•ˆè®°å½•: {len(enhanced_data)}")
    
    # æ‰“å°ä¸€äº›å…³é”®ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š å…³é”®ç‰¹å¾ç»Ÿè®¡:")
    key_features = [col for col in enhanced_data.columns if 'drift' in col or 'cumulative' in col][:5]
    for feat in key_features:
        if feat in enhanced_data.columns:
            print(f"   {feat}: mean={enhanced_data[feat].mean():.6f}, std={enhanced_data[feat].std():.6f}")
    
    return enhanced_data

# =====================================================================
# æ”¹è¿›çš„XGBoostæ£€æµ‹å™¨
# =====================================================================
def improved_xgboost_detection(enhanced_data, maneuver_times):
    """æ”¹è¿›çš„XGBoostæ£€æµ‹å™¨ï¼Œé’ˆå¯¹è½¨é“ç»´æŒæœºåŠ¨ä¼˜åŒ–"""
    print("\nğŸ¤– æ”¹è¿›çš„XGBoostæœºåŠ¨é¢„æµ‹")
    print("-" * 30)
    
    try:
        from models.hybrid.enhanced_xgboost_detector import ImprovedXGBoostDetector
        from models.hybrid.xgboost_detector import create_labels_for_split
        
        # 1. æ•°æ®åˆ’åˆ†
        split_ratio = 0.8
        split_index = int(len(enhanced_data) * split_ratio)
        train_period_data = enhanced_data.iloc[:split_index]
        test_data = enhanced_data.iloc[split_index:]
        
        print(f"   è®­ç»ƒå‘¨æœŸ: {len(train_period_data)} æ¡")
        print(f"   æµ‹è¯•å‘¨æœŸ: {len(test_data)} æ¡")
        
        # 2. æå–æ­£å¸¸æ•°æ®ç”¨äºè®­ç»ƒ
        train_labels = create_labels_for_split(
            train_period_data.index,
            maneuver_times,
            window=timedelta(days=1)
        )
        
        # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ä¸åŒçš„ç­–ç•¥ï¼šä¸åªç”¨æ­£å¸¸æ•°æ®è®­ç»ƒ
        # è€Œæ˜¯ç”¨æ‰€æœ‰æ•°æ®ï¼Œä½†ç»™æœºåŠ¨æœŸé—´çš„æ•°æ®æ›´é«˜çš„æƒé‡
        
        # 3. ä¸ºæœºåŠ¨æ£€æµ‹ä¼˜åŒ–çš„XGBoostå‚æ•°
        xgb_params = {
            'objective': 'reg:squarederror',  # é¢„æµ‹è·³å˜å¤§å°
            'n_estimators': 500,
            'learning_rate': 0.02,
            'max_depth': 8,  # æ›´æ·±çš„æ ‘æ¥æ•æ‰å¤æ‚çš„æ¼‚ç§»æ¨¡å¼
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 3,
            'gamma': 0.1,  # æ·»åŠ æ­£åˆ™åŒ–
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42
        }
        
        # 4. åˆå§‹åŒ–æ”¹è¿›çš„æ£€æµ‹å™¨
        detector = ImprovedXGBoostDetector(
            target_column='target_abs',  # ä½¿ç”¨ç»å¯¹å€¼ç›®æ ‡
            xgb_params=xgb_params,
            threshold_quantile=0.98,  # ä½¿ç”¨åˆ†ä½æ•°é˜ˆå€¼
            enable_threshold_optimization=True,
            enable_temporal_clustering=True,
            satellite_type='FY-4A',
            ultra_deep=True
        )
        
        # 5. è®­ç»ƒæ¨¡å‹
        print("   ğŸ”§ è®­ç»ƒæ”¹è¿›çš„XGBoostæ¨¡å‹...")
        
        # åˆ›å»ºæ ·æœ¬æƒé‡ï¼šæœºåŠ¨æœŸé—´çš„æ ·æœ¬æƒé‡æ›´é«˜
        sample_weights = np.ones(len(train_period_data))
        maneuver_indices = train_labels[train_labels == 1].index
        for idx in maneuver_indices:
            if idx in train_period_data.index:
                loc = train_period_data.index.get_loc(idx)
                # æœºåŠ¨å‰åçš„æ ·æœ¬ä¹Ÿå¾ˆé‡è¦
                for offset in range(-3, 4):  # å‰å3å¤©
                    if 0 <= loc + offset < len(sample_weights):
                        sample_weights[loc + offset] = 2.0
        
        # ä½¿ç”¨åŠ æƒè®­ç»ƒ
        detector.fit(
            train_features=train_period_data,
            satellite_name="Fengyun-4A",
            verbose=True,
        )
        
        # 6. æ£€æµ‹å¼‚å¸¸
        print("   ğŸ” æ£€æµ‹æµ‹è¯•é›†å¼‚å¸¸...")
        anomaly_indices, anomaly_scores = detector.detect_anomalies(test_data, return_scores=True)
        
        # 7. åŸºäºæ¼‚ç§»å‹åŠ›çš„åå¤„ç†
        print("   ğŸ”§ åŸºäºæ¼‚ç§»å‹åŠ›çš„åå¤„ç†...")
        
        # è·å–ç´¯ç§¯æ¼‚ç§»ç‰¹å¾
        drift_features = [col for col in test_data.columns if 'cumulative_drift' in col]
        if drift_features:
            # è®¡ç®—ç»¼åˆæ¼‚ç§»å‹åŠ›
            drift_pressure = test_data[drift_features].mean(axis=1)
            
            # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼šé«˜æ¼‚ç§»å‹åŠ›æ—¶é™ä½æ£€æµ‹é˜ˆå€¼
            pressure_percentile = drift_pressure.rank(pct=True)
            adjusted_threshold = detector.residual_threshold * (1 - 0.3 * pressure_percentile)
            
            # é‡æ–°æ£€æµ‹ï¼ˆè€ƒè™‘æ¼‚ç§»å‹åŠ›ï¼‰
            residuals = anomaly_scores    # è·å–æ®‹å·®
            adjusted_anomalies = []
            
            for i, (idx, residual) in enumerate(zip(test_data.index, residuals)):
                if idx in drift_pressure.index:
                    threshold = adjusted_threshold.iloc[drift_pressure.index.get_loc(idx)]
                    if residual > threshold:
                        adjusted_anomalies.append(i)
            
            print(f"   è°ƒæ•´å‰å¼‚å¸¸æ•°: {len(anomaly_indices)}")
            print(f"   è°ƒæ•´åå¼‚å¸¸æ•°: {len(adjusted_anomalies)}")
            
            anomaly_indices = adjusted_anomalies
        
        anomaly_timestamps = test_data.index[anomaly_indices]
        
        print(f"âœ… æ£€æµ‹å®Œæˆ:")
        print(f"   æ£€æµ‹åˆ° {len(anomaly_timestamps)} ä¸ªå¼‚å¸¸")
        print(f"   å¼‚å¸¸æ¯”ä¾‹: {len(anomaly_timestamps)/len(test_data)*100:.2f}%")
        
        return anomaly_timestamps, anomaly_scores, test_data, detector
        
    except Exception as e:
        print(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

# =====================================================================
# è¯„ä¼°å‡½æ•°
# =====================================================================
def evaluate_predictions(predictions, test_data, maneuver_times):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    print("\nğŸ“Š è¯„ä¼°é¢„æµ‹ç»“æœ")
    print("-" * 20)
    
    if predictions is None or len(predictions) == 0:
        print("âš ï¸ æ²¡æœ‰é¢„æµ‹ç»“æœ")
        return None
    
    from models.hybrid.xgboost_detector import create_labels_for_split
    
    # åˆ›å»ºçœŸå®æ ‡ç­¾
    true_labels = create_labels_for_split(
        test_data.index,
        maneuver_times,
        window=timedelta(days=1)
    )
    
    # åˆ›å»ºé¢„æµ‹æ ‡ç­¾
    pred_labels = pd.Series(0, index=test_data.index)
    for timestamp in predictions:
        if timestamp in pred_labels.index:
            pred_labels.loc[timestamp] = 1
    
    # è®¡ç®—æŒ‡æ ‡
    precision = precision_score(true_labels, pred_labels, zero_division=0)
    recall = recall_score(true_labels, pred_labels, zero_division=0)
    f1 = f1_score(true_labels, pred_labels, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()
    
    print(f"âœ… æ€§èƒ½æŒ‡æ ‡:")
    print(f"   ç²¾ç¡®ç‡: {precision:.3f}")
    print(f"   å¬å›ç‡: {recall:.3f}")
    print(f"   F1åˆ†æ•°: {f1:.3f}")
    print(f"   çœŸæ­£ä¾‹(TP): {tp}, å‡æ­£ä¾‹(FP): {fp}")
    print(f"   çœŸè´Ÿä¾‹(TN): {tn}, å‡è´Ÿä¾‹(FN): {fn}")
    
    # åˆ†æé¢„æµ‹æ—¶æœº
    print("\nğŸ“… é¢„æµ‹æ—¶æœºåˆ†æ:")
    if len(predictions) > 0 and len(maneuver_times) > 0:
        # è®¡ç®—æ¯ä¸ªé¢„æµ‹ä¸æœ€è¿‘æœºåŠ¨çš„æ—¶é—´å·®
        time_diffs = []
        for pred_time in predictions:
            min_diff = min([abs((pred_time - m_time).days) for m_time in maneuver_times])
            time_diffs.append(min_diff)
        
        time_diffs = np.array(time_diffs)
        print(f"   æå‰é¢„æµ‹ï¼ˆ<3å¤©ï¼‰: {np.sum(time_diffs < 3)} ä¸ª")
        print(f"   å‡†ç¡®é¢„æµ‹ï¼ˆÂ±1å¤©ï¼‰: {np.sum(time_diffs <= 1)} ä¸ª")
        print(f"   å»¶è¿Ÿé¢„æµ‹ï¼ˆ>3å¤©ï¼‰: {np.sum(time_diffs > 3)} ä¸ª")
        print(f"   å¹³å‡æ—¶é—´å·®: {np.mean(time_diffs):.1f} å¤©")
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn
    }

# =====================================================================
# å¯è§†åŒ–å‡½æ•°
# =====================================================================
def visualize_results(enhanced_data, predictions, maneuver_times, detector):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœå’Œæ¼‚ç§»æ¨¡å¼"""
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–...")
    
    fig, axes = plt.subplots(4, 1, figsize=(15, 12))
    fig.suptitle('Fengyun-4A è½¨é“ç»´æŒæœºåŠ¨é¢„æµ‹åˆ†æ', fontsize=16)
    
    # 1. Mean motionå’Œé¢„æµ‹
    ax1 = axes[0]
    ax1.plot(enhanced_data.index, enhanced_data['mean_motion'], 'b-', alpha=0.7, label='Mean Motion')
    
    # æ ‡è®°çœŸå®æœºåŠ¨
    for m_time in maneuver_times:
        if enhanced_data.index[0] <= m_time <= enhanced_data.index[-1]:
            ax1.axvline(m_time, color='red', alpha=0.5, linestyle='--', label='çœŸå®æœºåŠ¨' if m_time == maneuver_times[0] else '')
    
    # æ ‡è®°é¢„æµ‹
    if predictions is not None:
        for pred_time in predictions:
            ax1.axvline(pred_time, color='green', alpha=0.5, linestyle=':', label='é¢„æµ‹æœºåŠ¨' if pred_time == predictions[0] else '')
    
    ax1.set_ylabel('Mean Motion')
    ax1.legend()
    ax1.set_title('Mean Motion å˜åŒ–å’ŒæœºåŠ¨äº‹ä»¶')
    
    # 2. æ¼‚ç§»å‹åŠ›
    ax2 = axes[1]
    drift_cols = [col for col in enhanced_data.columns if 'cumulative_drift' in col and 'mean_motion' in col]
    if drift_cols:
        drift_pressure = enhanced_data[drift_cols[0]]
        ax2.plot(enhanced_data.index, drift_pressure, 'orange', alpha=0.7, label='ç´¯ç§¯æ¼‚ç§»å‹åŠ›')
        ax2.set_ylabel('æ¼‚ç§»å‹åŠ›')
        ax2.legend()
        ax2.set_title('Mean Motion ç´¯ç§¯æ¼‚ç§»å‹åŠ›')
    
    # 3. é¢„æµ‹ç›®æ ‡ï¼ˆè·³å˜å¤§å°ï¼‰
    ax3 = axes[2]
    if 'target_abs' in enhanced_data.columns:
        ax3.plot(enhanced_data.index, enhanced_data['target_abs'], 'purple', alpha=0.5, label='å®é™…è·³å˜å¤§å°')
        ax3.set_ylabel('è·³å˜å¤§å°')
        ax3.legend()
        ax3.set_title('Mean Motion è·³å˜å¤§å°ï¼ˆç»å¯¹å€¼ï¼‰')
    
    # 4. å¼‚å¸¸åˆ†æ•°
    ax4 = axes[3]
    if detector is not None and hasattr(detector, 'model'):
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è·å–å®Œæ•´çš„å¼‚å¸¸åˆ†æ•°
        ax4.set_ylabel('å¼‚å¸¸åˆ†æ•°')
        ax4.set_title('æ¨¡å‹å¼‚å¸¸åˆ†æ•°')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    output_dir = 'outputs/fengyun4a_improved'
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(os.path.join(output_dir, 'drift_analysis.png'), dpi=300, bbox_inches='tight')
    print(f"   å›¾ç‰‡å·²ä¿å­˜åˆ°: {output_dir}/drift_analysis.png")
    
    plt.close()

# =====================================================================
# ä¸»å‡½æ•°
# =====================================================================
def main():
    """ä¸»å‡½æ•°"""
    print(f"ğŸ“‚ å·¥ä½œç›®å½•: {project_root}")
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    from data.loader import SatelliteDataLoader
    
    loader = SatelliteDataLoader(data_dir="data")
    tle_data, maneuver_times = loader.load_satellite_data("Fengyun-4A")
    
    if tle_data is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    print(f"âœ… åŠ è½½ {len(tle_data)} æ¡TLEè®°å½•")
    print(f"âœ… åŠ è½½ {len(maneuver_times)} ä¸ªæœºåŠ¨äº‹ä»¶")
    
    # 2. åˆ›å»ºæ¼‚ç§»ç‰¹å¾
    enhanced_data = create_drift_features(tle_data)
    if enhanced_data is None:
        print("âŒ ç‰¹å¾åˆ›å»ºå¤±è´¥")
        return
    
    # 3. è¿è¡Œæ”¹è¿›çš„XGBoostæ£€æµ‹
    predictions, scores, test_data, detector = improved_xgboost_detection(
        enhanced_data, maneuver_times
    )
    
    # 4. è¯„ä¼°ç»“æœ
    if predictions is not None:
        metrics = evaluate_predictions(predictions, test_data, maneuver_times)
        
        # 5. å¯è§†åŒ–
        visualize_results(enhanced_data, predictions, maneuver_times, detector)
    
    print("\nâœ… æ”¹è¿›æ–¹æ¡ˆæ‰§è¡Œå®Œæˆï¼")
    
    # 6. ä¿å­˜ç»“æœæŠ¥å‘Š
    if predictions is not None and metrics is not None:
        report_path = 'outputs/fengyun4a_improved/performance_report.txt'
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w') as f:
            f.write("Fengyun-4A æ”¹è¿›çš„æœºåŠ¨é¢„æµ‹ç»“æœ\n")
            f.write("=" * 50 + "\n\n")
            f.write("1. æ•°æ®æ¦‚å†µ:\n")
            f.write(f"   TLEè®°å½•æ•°: {len(tle_data)}\n")
            f.write(f"   å¢å¼ºç‰¹å¾æ•°: {len(enhanced_data.columns)}\n")
            f.write(f"   æœºåŠ¨äº‹ä»¶æ•°: {len(maneuver_times)}\n")
            f.write(f"\n2. æ£€æµ‹ç»“æœ:\n")
            f.write(f"   æµ‹è¯•é›†å¤§å°: {len(test_data)}\n")
            f.write(f"   æ£€æµ‹å¼‚å¸¸æ•°: {len(predictions)}\n")
            f.write(f"\n3. æ€§èƒ½æŒ‡æ ‡:\n")
            for key, value in metrics.items():
                f.write(f"   {key}: {value}\n")
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

if __name__ == "__main__":
    main()