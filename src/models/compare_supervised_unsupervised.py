# scripts/compare_supervised_unsupervised.py
"""
æœ‰ç›‘ç£ä¸æ— ç›‘ç£XGBoostæœºåŠ¨æ£€æµ‹æ–¹æ³•å¯¹æ¯”

è¯¥è„šæœ¬å¯¹æ¯”åˆ†ææœ‰ç›‘ç£å’Œæ— ç›‘ç£ä¸¤ç§æ–¹æ³•åœ¨ç›¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Tuple

# è®¾ç½®é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from src.data.loader import SatelliteDataLoader
from src.data.feature_engineer import create_drift_enhanced_features
from src.models.unsupervised.xgboost_unsupervised_adapter import create_unsupervised_detector
from src.models.hybrid.enhanced_xgboost_detector import ImprovedXGBoostDetector
from src.models.hybrid.xgboost_detector import create_labels_for_split


class MethodComparison:
    """æ–¹æ³•å¯¹æ¯”å®éªŒç±»"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.results = {
            'supervised': {},
            'unsupervised': {}
        }
        
    def run_comparison(self):
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        print("\n" + "="*70)
        print("ğŸ”¬ æœ‰ç›‘ç£ vs æ— ç›‘ç£ XGBoost æœºåŠ¨æ£€æµ‹å¯¹æ¯”å®éªŒ")
        print("="*70)
        
        # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
        tle_data, maneuver_times, enhanced_data = self._prepare_data()
        
        # 2. æ•°æ®åˆ’åˆ†
        train_data, test_data = self._split_data(enhanced_data)
        
        # 3. è¿è¡Œæœ‰ç›‘ç£æ–¹æ³•
        print("\n" + "-"*50)
        print("ğŸ“š è¿è¡Œæœ‰ç›‘ç£æ–¹æ³•...")
        supervised_results = self._run_supervised(train_data, test_data, maneuver_times)
        self.results['supervised'] = supervised_results
        
        # 4. è¿è¡Œæ— ç›‘ç£æ–¹æ³•
        print("\n" + "-"*50)
        print("ğŸ”“ è¿è¡Œæ— ç›‘ç£æ–¹æ³•...")
        unsupervised_results = self._run_unsupervised(train_data, test_data, maneuver_times)
        self.results['unsupervised'] = unsupervised_results
        
        # 5. å¯¹æ¯”åˆ†æ
        print("\n" + "-"*50)
        self._compare_results()
        
        # 6. å¯è§†åŒ–å¯¹æ¯”
        self._visualize_comparison(test_data, maneuver_times)
        
        # 7. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_comparison_report()
        
        return self.results
    
    def _prepare_data(self):
        """å‡†å¤‡æ•°æ®"""
        print("\nğŸ“ æ•°æ®å‡†å¤‡...")
        
        # åŠ è½½æ•°æ®
        loader = SatelliteDataLoader(data_dir=self.config['data_dir'])
        tle_data, maneuver_times = loader.load_satellite_data(
            self.config['satellite_name']
        )
        
        # ç‰¹å¾å·¥ç¨‹
        enhanced_data = create_drift_enhanced_features(
            tle_data,
            scaling_factor=self.config['target_scaling_factor']
        )
        
        print(f"   - æ€»æ ·æœ¬æ•°: {len(enhanced_data)}")
        print(f"   - ç‰¹å¾æ•°: {len(enhanced_data.columns)}")
        print(f"   - å·²çŸ¥æœºåŠ¨æ•°: {len(maneuver_times)}")
        
        return tle_data, maneuver_times, enhanced_data
    
    def _split_data(self, enhanced_data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """åˆ’åˆ†æ•°æ®é›†"""
        split_idx = int(len(enhanced_data) * self.config['train_ratio'])
        train_data = enhanced_data.iloc[:split_idx]
        test_data = enhanced_data.iloc[split_idx:]
        
        print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"   - è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        print(f"   - æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
        
        return train_data, test_data
    
    def _run_supervised(self, train_data: pd.DataFrame, test_data: pd.DataFrame, 
                       maneuver_times: list) -> Dict:
        """è¿è¡Œæœ‰ç›‘ç£æ–¹æ³•"""
        
        # åˆ›å»ºè®­ç»ƒæ ‡ç­¾
        train_labels = create_labels_for_split(
            train_data.index,
            maneuver_times,
            timedelta(days=self.config['label_window_days'])
        )
        
        # æå–æ­£å¸¸æ•°æ®ç”¨äºè®­ç»ƒ
        normal_train_data = train_data[train_labels == 0]
        
        print(f"   - æ­£å¸¸è®­ç»ƒæ ·æœ¬: {len(normal_train_data)}")
        print(f"   - å¼‚å¸¸è®­ç»ƒæ ·æœ¬: {train_labels.sum()}")
        
        # é…ç½®å¹¶è®­ç»ƒæœ‰ç›‘ç£æ£€æµ‹å™¨
        xgb_params = {
            'objective': 'reg:squarederror',
            'n_estimators': 300,
            'learning_rate': 0.03,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }
        
        detector = ImprovedXGBoostDetector(
            target_column='target',
            xgb_params=xgb_params,
            threshold_quantile=0.995,
            enable_threshold_optimization=True,
            enable_temporal_clustering=True,
            satellite_type='FY-4A'
        )
        
        # è®­ç»ƒæ¨¡å‹
        detector.fit(
            train_features=normal_train_data,
            satellite_name="Fengyun-4A",
            verbose=False
        )
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies, scores = detector.detect_anomalies(test_data, return_scores=True)
        
        # è¯„ä¼°ç»“æœ
        metrics = self._evaluate_predictions(test_data, anomalies, maneuver_times)
        
        return {
            'anomalies': anomalies,
            'scores': scores,
            'metrics': metrics,
            'detector': detector
        }
    
    def _run_unsupervised(self, train_data: pd.DataFrame, test_data: pd.DataFrame,
                         maneuver_times: list) -> Dict:
        """è¿è¡Œæ— ç›‘ç£æ–¹æ³•"""
        
        # åˆ›å»ºæ— ç›‘ç£æ£€æµ‹å™¨
        detector = create_unsupervised_detector({
            'threshold_method': 'mad',
            'threshold_factor': 3.5,
            'min_segment_size': 3,
            'max_gap_days': 1.5,
            'enable_drift_adjustment': True
        })
        
        # è®­ç»ƒï¼ˆä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼‰
        detector.fit(train_data, target_column='target')
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies, scores = detector.detect_anomalies(test_data, return_scores=True)
        
        # è¯„ä¼°ç»“æœ
        metrics = self._evaluate_predictions(test_data, anomalies, maneuver_times)
        
        return {
            'anomalies': anomalies,
            'scores': scores,
            'metrics': metrics,
            'detector': detector
        }
    
    def _evaluate_predictions(self, test_data: pd.DataFrame, anomalies: pd.Index,
                            maneuver_times: list) -> Dict:
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        
        # åˆ›å»ºçœŸå®æ ‡ç­¾
        test_labels = create_labels_for_split(
            test_data.index,
            maneuver_times,
            timedelta(days=self.config['label_window_days'])
        )
        
        # åˆ›å»ºé¢„æµ‹æ ‡ç­¾
        pred_labels = pd.Series(0, index=test_data.index)
        pred_labels[anomalies] = 1
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        tp = ((pred_labels == 1) & (test_labels == 1)).sum()
        fp = ((pred_labels == 1) & (test_labels == 0)).sum()
        tn = ((pred_labels == 0) & (test_labels == 0)).sum()
        fn = ((pred_labels == 0) & (test_labels == 1)).sum()
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,
            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,
            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'f1': 0,
            'accuracy': (tp + tn) / (tp + tn + fp + fn)
        }
        
        if metrics['precision'] + metrics['recall'] > 0:
            metrics['f1'] = 2 * (metrics['precision'] * metrics['recall']) / \
                           (metrics['precision'] + metrics['recall'])
        
        return metrics
    
    def _compare_results(self):
        """å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„ç»“æœ"""
        print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print("-" * 50)
        print(f"{'æŒ‡æ ‡':<10} {'æœ‰ç›‘ç£':<15} {'æ— ç›‘ç£':<15} {'å·®å¼‚':<10}")
        print("-" * 50)
        
        metrics_to_compare = ['precision', 'recall', 'f1', 'accuracy']
        
        for metric in metrics_to_compare:
            sup_val = self.results['supervised']['metrics'][metric]
            unsup_val = self.results['unsupervised']['metrics'][metric]
            diff = sup_val - unsup_val
            
            print(f"{metric:<10} {sup_val:<15.3f} {unsup_val:<15.3f} {diff:+.3f}")
        
        print("-" * 50)
        
        # æ£€æµ‹æ•°é‡å¯¹æ¯”
        sup_count = len(self.results['supervised']['anomalies'])
        unsup_count = len(self.results['unsupervised']['anomalies'])
        
        print(f"\næ£€æµ‹åˆ°çš„å¼‚å¸¸æ•°:")
        print(f"  - æœ‰ç›‘ç£: {sup_count}")
        print(f"  - æ— ç›‘ç£: {unsup_count}")
        
        # è®¡ç®—é‡å 
        overlap = len(set(self.results['supervised']['anomalies']) & 
                     set(self.results['unsupervised']['anomalies']))
        
        print(f"  - é‡å å¼‚å¸¸: {overlap}")
        print(f"  - Jaccardç›¸ä¼¼åº¦: {overlap / (sup_count + unsup_count - overlap):.3f}")
    
    def _visualize_comparison(self, test_data: pd.DataFrame, maneuver_times: list):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        print("\nğŸ“ˆ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
        
        fig, axes = plt.subplots(4, 1, figsize=(15, 16))
        
        # 1. Mean Motion ä¸ä¸¤ç§æ–¹æ³•çš„æ£€æµ‹ç»“æœ
        ax = axes[0]
        ax.plot(test_data.index, test_data['mean_motion'], 'k-', alpha=0.5, label='Mean Motion')
        
        # æœ‰ç›‘ç£æ£€æµ‹ç»“æœ
        sup_anomalies = self.results['supervised']['anomalies']
        if len(sup_anomalies) > 0:
            sup_data = test_data.loc[sup_anomalies]
            ax.scatter(sup_data.index, sup_data['mean_motion'], 
                      color='blue', s=60, alpha=0.7, marker='o', label='æœ‰ç›‘ç£æ£€æµ‹')
        
        # æ— ç›‘ç£æ£€æµ‹ç»“æœ
        unsup_anomalies = self.results['unsupervised']['anomalies']
        if len(unsup_anomalies) > 0:
            unsup_data = test_data.loc[unsup_anomalies]
            ax.scatter(unsup_data.index, unsup_data['mean_motion'], 
                      color='red', s=40, alpha=0.7, marker='s', label='æ— ç›‘ç£æ£€æµ‹')
        
        # çœŸå®æœºåŠ¨
        for maneuver_time in maneuver_times:
            if test_data.index[0] <= maneuver_time <= test_data.index[-1]:
                ax.axvline(x=maneuver_time, color='green', linestyle='--', 
                          alpha=0.5, linewidth=1)
        
        ax.set_xlabel('æ—¶é—´')
        ax.set_ylabel('Mean Motion')
        ax.set_title('Mean Motion æ—¶åºä¸æ£€æµ‹ç»“æœå¯¹æ¯”')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”
        ax = axes[1]
        
        # æœ‰ç›‘ç£åˆ†æ•°
        sup_scores = self.results['supervised']['scores']
        ax.hist(sup_scores, bins=50, alpha=0.5, color='blue', 
                label='æœ‰ç›‘ç£', density=True)
        
        # æ— ç›‘ç£åˆ†æ•°
        unsup_scores = self.results['unsupervised']['scores']
        ax.hist(unsup_scores, bins=50, alpha=0.5, color='red', 
                label='æ— ç›‘ç£', density=True)
        
        ax.set_xlabel('å¼‚å¸¸åˆ†æ•°')
        ax.set_ylabel('å¯†åº¦')
        ax.set_title('å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        ax = axes[2]
        
        metrics = ['Precision', 'Recall', 'F1', 'Accuracy']
        sup_values = [self.results['supervised']['metrics'][m.lower()] for m in metrics]
        unsup_values = [self.results['unsupervised']['metrics'][m.lower()] for m in metrics]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax.bar(x - width/2, sup_values, width, label='æœ‰ç›‘ç£', color='blue', alpha=0.7)
        ax.bar(x + width/2, unsup_values, width, label='æ— ç›‘ç£', color='red', alpha=0.7)
        
        ax.set_xlabel('æŒ‡æ ‡')
        ax.set_ylabel('å€¼')
        ax.set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(metrics)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        
        # 4. æ··æ·†çŸ©é˜µå¯¹æ¯”
        ax = axes[3]
        ax.axis('off')
        
        # åˆ›å»ºæ··æ·†çŸ©é˜µè¡¨æ ¼
        sup_m = self.results['supervised']['metrics']
        unsup_m = self.results['unsupervised']['metrics']
        
        table_data = [
            ['æ–¹æ³•', 'TP', 'FP', 'TN', 'FN'],
            ['æœ‰ç›‘ç£', sup_m['tp'], sup_m['fp'], sup_m['tn'], sup_m['fn']],
            ['æ— ç›‘ç£', unsup_m['tp'], unsup_m['fp'], unsup_m['tn'], unsup_m['fn']]
        ]
        
        table = ax.table(cellText=table_data, loc='center', cellLoc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1.2, 1.5)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(len(table_data)):
            for j in range(len(table_data[0])):
                cell = table[(i, j)]
                if i == 0:
                    cell.set_facecolor('#40466e')
                    cell.set_text_props(weight='bold', color='white')
                else:
                    cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')
        
        ax.set_title('æ··æ·†çŸ©é˜µå¯¹æ¯”', pad=20)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        output_path = os.path.join(self.config['output_dir'], 'method_comparison.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   - å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_path}")
    
    def _generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        report_path = os.path.join(self.config['output_dir'], 'comparison_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("æœ‰ç›‘ç£ vs æ— ç›‘ç£ XGBoost æœºåŠ¨æ£€æµ‹å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"å«æ˜Ÿ: {self.config['satellite_name']}\n")
            f.write(f"è®­ç»ƒé›†æ¯”ä¾‹: {self.config['train_ratio']*100:.0f}%\n\n")
            
            # æ€§èƒ½å¯¹æ¯”è¡¨
            f.write("1. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:\n")
            f.write("-" * 50 + "\n")
            f.write(f"{'æŒ‡æ ‡':<15} {'æœ‰ç›‘ç£':<15} {'æ— ç›‘ç£':<15} {'ä¼˜åŠ¿':<10}\n")
            f.write("-" * 50 + "\n")
            
            metrics = ['precision', 'recall', 'f1', 'accuracy']
            for metric in metrics:
                sup = self.results['supervised']['metrics'][metric]
                unsup = self.results['unsupervised']['metrics'][metric]
                better = "æœ‰ç›‘ç£" if sup > unsup else "æ— ç›‘ç£" if unsup > sup else "ç›¸åŒ"
                f.write(f"{metric:<15} {sup:<15.3f} {unsup:<15.3f} {better:<10}\n")
            
            f.write("\n2. æ£€æµ‹ç»“æœå¯¹æ¯”:\n")
            sup_count = len(self.results['supervised']['anomalies'])
            unsup_count = len(self.results['unsupervised']['anomalies'])
            overlap = len(set(self.results['supervised']['anomalies']) & 
                         set(self.results['unsupervised']['anomalies']))
            
            f.write(f"   - æœ‰ç›‘ç£æ£€æµ‹æ•°: {sup_count}\n")
            f.write(f"   - æ— ç›‘ç£æ£€æµ‹æ•°: {unsup_count}\n")
            f.write(f"   - é‡å æ£€æµ‹æ•°: {overlap}\n")
            f.write(f"   - Jaccardç›¸ä¼¼åº¦: {overlap / (sup_count + unsup_count - overlap):.3f}\n")
            
            f.write("\n3. ä¼˜ç¼ºç‚¹åˆ†æ:\n")
            f.write("\næœ‰ç›‘ç£æ–¹æ³•:\n")
            f.write("   ä¼˜ç‚¹:\n")
            f.write("   - å¯ä»¥åˆ©ç”¨å·²çŸ¥çš„æœºåŠ¨æ ‡ç­¾è¿›è¡Œè®­ç»ƒ\n")
            f.write("   - é€šå¸¸å…·æœ‰æ›´é«˜çš„ç²¾ç¡®ç‡\n")
            f.write("   - å¯ä»¥è¿›è¡Œé˜ˆå€¼ä¼˜åŒ–\n")
            f.write("   ç¼ºç‚¹:\n")
            f.write("   - éœ€è¦æ ‡æ³¨æ•°æ®\n")
            f.write("   - å¯èƒ½å¯¹æ–°å‹æœºåŠ¨æ¨¡å¼æ³›åŒ–èƒ½åŠ›æœ‰é™\n")
            
            f.write("\næ— ç›‘ç£æ–¹æ³•:\n")
            f.write("   ä¼˜ç‚¹:\n")
            f.write("   - ä¸éœ€è¦æ ‡æ³¨æ•°æ®\n")
            f.write("   - å¯ä»¥å‘ç°æœªçŸ¥çš„å¼‚å¸¸æ¨¡å¼\n")
            f.write("   - æ›´å®¹æ˜“éƒ¨ç½²åˆ°æ–°å«æ˜Ÿ\n")
            f.write("   ç¼ºç‚¹:\n")
            f.write("   - é˜ˆå€¼è®¾ç½®å¯èƒ½éœ€è¦æ›´å¤šè°ƒè¯•\n")
            f.write("   - å¯èƒ½äº§ç”Ÿæ›´å¤šè¯¯æŠ¥\n")
            
            f.write("\n4. å»ºè®®:\n")
            if self.results['supervised']['metrics']['f1'] > self.results['unsupervised']['metrics']['f1'] + 0.1:
                f.write("   - å½“å‰æ•°æ®é›†ä¸Šï¼Œæœ‰ç›‘ç£æ–¹æ³•è¡¨ç°æ˜æ˜¾æ›´å¥½\n")
                f.write("   - å»ºè®®åœ¨æœ‰å……è¶³æ ‡æ³¨æ•°æ®æ—¶ä½¿ç”¨æœ‰ç›‘ç£æ–¹æ³•\n")
            elif self.results['unsupervised']['metrics']['f1'] > self.results['supervised']['metrics']['f1'] + 0.1:
                f.write("   - å½“å‰æ•°æ®é›†ä¸Šï¼Œæ— ç›‘ç£æ–¹æ³•è¡¨ç°æ›´å¥½\n")
                f.write("   - å¯èƒ½æ˜¯ç”±äºè®­ç»ƒæ•°æ®ä¸­çš„æ ‡æ³¨ä¸å®Œæ•´\n")
            else:
                f.write("   - ä¸¤ç§æ–¹æ³•è¡¨ç°ç›¸è¿‘\n")
                f.write("   - å»ºè®®æ ¹æ®å®é™…åº”ç”¨åœºæ™¯é€‰æ‹©\n")
                f.write("   - å¯ä»¥è€ƒè™‘é›†æˆä¸¤ç§æ–¹æ³•çš„ç»“æœ\n")
        
        print(f"   - æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    config = {
        'satellite_name': 'Fengyun-4A',
        'data_dir': 'data',
        'output_dir': 'outputs/method_comparison',
        'train_ratio': 0.7,
        'target_scaling_factor': 1e8,
        'label_window_days': 1
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    comparison = MethodComparison(config)
    results = comparison.run_comparison()
    
    print("\n" + "="*70)
    print("âœ… å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print("="*70)
    
    return results


if __name__ == "__main__":
    main()