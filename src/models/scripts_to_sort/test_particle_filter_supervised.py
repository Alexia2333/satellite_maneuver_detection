# scripts/test_particle_filter_supervised.py
"""
å«æ˜ŸæœºåŠ¨æ£€æµ‹ç²’å­æ»¤æ³¢å™¨ï¼ˆParticle Filterï¼‰- ã€æœ‰ç›‘ç£ã€‘æ€§èƒ½æµ‹è¯•è„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨è¯„ä¼° `ParticleFilterDetector` åœ¨ã€æœ‰ç›‘ç£ã€‘æ¨¡å¼ä¸‹çš„æ€§èƒ½ã€‚
ä¸æ— ç›‘ç£è„šæœ¬çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºè®­ç»ƒé˜¶æ®µï¼š
1.  **å™ªå£°ä¼°è®¡**: åªåœ¨è®­ç»ƒé›†ä¸­è¢«æ ‡è®°ä¸ºâ€œæ­£å¸¸â€ï¼ˆæ— æœºåŠ¨ï¼‰çš„æ—¶é—´æ®µå†…ä¼°è®¡å™ªå£°åæ–¹å·®(Q, R)ã€‚
2.  **é˜ˆå€¼ä¼˜åŒ–**: åœ¨å®Œæ•´çš„è®­ç»ƒé›†ä¸Šè®¡ç®—å¼‚å¸¸åˆ†æ•°ï¼Œå¹¶å¯»æ‰¾èƒ½æœ€å¤§åŒ–F1åˆ†æ•°çš„æœ€ä½³å¼‚å¸¸é˜ˆå€¼ã€‚
"""
import os
import sys
import pandas as pd
import numpy as np
from datetime import timedelta
import matplotlib.pyplot as plt
import json
import warnings
from sklearn.metrics import f1_score, precision_recall_curve

# --- é¡¹ç›®è·¯å¾„è®¾ç½® ---
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
from data.loader import SatelliteDataLoader
from data.mean_elements_processor import MeanElementsProcessor
from models.unsupervised.particle_filter import ParticleFilterDetector

warnings.filterwarnings('ignore', category=UserWarning)

class ParticleFilterSupervisedExperiment:
    """å°è£…ç²’å­æ»¤æ³¢å™¨ã€æœ‰ç›‘ç£ã€‘æ€§èƒ½æµ‹è¯•çš„å®éªŒç±»"""

    def __init__(self, config: dict):
        self.config = config
        self.loader = SatelliteDataLoader(data_dir=config.get('data_dir', 'data'))
        self.processor = MeanElementsProcessor(satellite_type='auto', outlier_detection=True)
        self.detector = None
        self.results = {}
        
        os.makedirs(self.config['output_dir'], exist_ok=True)
        print(f"ğŸš€ åˆå§‹åŒ–ã€æœ‰ç›‘ç£ã€‘ç²’å­æ»¤æ³¢å™¨å®éªŒï¼Œè¾“å‡ºå°†ä¿å­˜è‡³: {self.config['output_dir']}")

    def _train_detector(self, train_data: pd.DataFrame, all_maneuver_times: list):
        """
        åœ¨è®­ç»ƒæ•°æ®ä¸Šã€æœ‰ç›‘ç£åœ°ã€‘æ‹Ÿåˆç²’å­æ»¤æ³¢å™¨æ£€æµ‹å™¨ã€‚
        """
        print("\n[4/7] ğŸ§  è®­ç»ƒç²’å­æ»¤æ³¢å™¨æ£€æµ‹å™¨ (SUPERVISED)...")

        # --- æ­¥éª¤ 1: ä»â€œæ­£å¸¸â€æ•°æ®ä¸­ä¼°è®¡å™ªå£° ---
        print("   -> æ­¥éª¤ 1: ä»æ— æœºåŠ¨æ•°æ®ä¸­ä¼°è®¡å™ªå£° (Q å’Œ R)...")
        maneuver_window = timedelta(days=self.config.get('label_window_days', 2.0))
        
        # åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œæ ‡è®°å‡ºè®­ç»ƒæ•°æ®ä¸­çš„æ­£å¸¸æ—¶æ®µ
        is_normal_mask = pd.Series(True, index=train_data.index)
        for m_time in all_maneuver_times:
            if train_data.index.min() <= m_time <= train_data.index.max():
                start_window = m_time - maneuver_window
                end_window = m_time + maneuver_window
                is_normal_mask.loc[start_window:end_window] = False
        
        normal_train_data = train_data[is_normal_mask]
        print(f"      -> ä½¿ç”¨ {len(normal_train_data)} / {len(train_data)} æ¡â€œçº¯å‡€â€æ•°æ®è¿›è¡Œå™ªå£°ä¼°è®¡ã€‚")

        # ä½¿ç”¨ä¸€ä¸ªä¸´æ—¶æ£€æµ‹å™¨åœ¨â€œçº¯å‡€â€æ•°æ®ä¸Šè¿›è¡Œæ‹Ÿåˆï¼Œä»¥è·å¾—Qå’ŒR
        temp_detector = ParticleFilterDetector(
            n_particles=self.config.get('n_particles', 250),
            noise_scaling_factor=self.config.get('noise_scaling_factor', 1.0)
        )
        temp_detector.fit(normal_train_data)
        
        # --- æ­¥éª¤ 2: åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼ ---
        print("\n   -> æ­¥éª¤ 2: åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼...")
        
        # åˆ›å»ºæœ€ç»ˆçš„æ£€æµ‹å™¨ï¼Œå¹¶åŠ è½½ä¸Šä¸€æ­¥å¾—åˆ°çš„Qå’ŒR
        self.detector = ParticleFilterDetector(n_particles=self.config.get('n_particles', 250))
        self.detector.estimated_Q = temp_detector.estimated_Q
        self.detector.estimated_R = temp_detector.estimated_R
        self.detector.state_columns = temp_detector.state_columns
        self.detector.is_trained = True
        
        # åœ¨ã€å®Œæ•´ã€‘è®­ç»ƒé›†ä¸Šè®¡ç®—å¼‚å¸¸åˆ†æ•°
        # å°†é˜ˆå€¼è®¾ä¸ºè´Ÿæ— ç©·å¤§ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬èƒ½å¾—åˆ°æ‰€æœ‰åˆ†æ•°è€Œä¸è¿›è¡Œä»»ä½•åˆ¤æ–­
        self.detector.anomaly_threshold = -np.inf 
        _, train_scores = self.detector.detect_anomalies(train_data)

        # ä¸ºè®­ç»ƒé›†åˆ›å»ºçœŸå€¼æ ‡ç­¾ (0=æ­£å¸¸, 1=æœºåŠ¨)
        # æ³¨æ„ï¼šåˆ†æ•°çš„é•¿åº¦æ¯”æ•°æ®å°‘1ï¼Œå› ä¸ºå®ƒæ˜¯ä»ç¬¬äºŒä¸ªç‚¹å¼€å§‹çš„
        true_labels_train = pd.Series(0, index=train_data.index[1:])
        label_window_eval = timedelta(days=self.config.get('label_window_days_eval', 1.0))
        for m_time in all_maneuver_times:
            mask = (true_labels_train.index >= m_time - label_window_eval) & \
                   (true_labels_train.index <= m_time + label_window_eval)
            if mask.any():
                true_labels_train[mask] = 1

        # ä½¿ç”¨ precision_recall_curve å¯»æ‰¾æœ€ä½³é˜ˆå€¼
        precision, recall, thresholds = precision_recall_curve(true_labels_train, train_scores)
        
        # è®¡ç®—F1åˆ†æ•°, é¿å…é™¤ä»¥é›¶çš„é”™è¯¯
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
        
        # æ‰¾åˆ°æœ€å¤§F1åˆ†æ•°å¯¹åº”çš„ç´¢å¼•å’Œé˜ˆå€¼
        # æ³¨æ„ï¼šthresholdsçš„é•¿åº¦æ¯”precision/recall/f1_scoreså¤šä¸€ä¸ª
        best_f1_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_f1_idx]
        best_f1 = f1_scores[best_f1_idx]
        
        print(f"      -> åœ¨è®­ç»ƒé›†ä¸Šæ‰¾åˆ°çš„æœ€ä¼˜é˜ˆå€¼: {best_threshold:.4f} (å¯è¾¾åˆ° F1-Score: {best_f1:.4f})")
        
        # å°†æ‰¾åˆ°çš„æœ€ä¼˜é˜ˆå€¼è®¾ä¸ºæ¨¡å‹çš„æœ€ç»ˆé˜ˆå€¼
        self.detector.anomaly_threshold = best_threshold
        print(f"   -> æœ€ç»ˆæ¨¡å‹é…ç½®: é˜ˆå€¼={self.detector.anomaly_threshold:.4f}, ç²’å­æ•°={self.detector.n_particles}")



    # =====================================================================
    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜æˆ–ç¨ä½œè°ƒæ•´
    # =====================================================================
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹"""
        print("\n" + "="*60)
        print(f"ğŸ›°ï¸  å¼€å§‹å¯¹å«æ˜Ÿ '{self.config['satellite_name']}' è¿›è¡Œã€æœ‰ç›‘ç£ã€‘æ€§èƒ½æµ‹è¯•")
        print("="*60)

        # 1. åŠ è½½æ•°æ®
        tle_data, maneuver_times = self._load_data()
        
        # 2. æ•°æ®å¤„ç†
        processed_data = self._process_data(tle_data)
        
        print("\n[*] æ ¹æ®é…ç½®ç­›é€‰è¾“å…¥æ•°æ®...")
        if self.config.get('use_only_mean_motion', False):
            print("   -> 'use_only_mean_motion' å¼€å¯ï¼Œæ¨¡å‹å°†åªä½¿ç”¨ [mean_motion] ç»´åº¦ã€‚")
            pf_input_data = processed_data[['mean_motion']]
        else:
            print("   -> 'use_only_mean_motion' å…³é—­ï¼Œä½¿ç”¨å¤šç»´æ ¸å¿ƒæ ¹æ•°ã€‚")
            core_elements = ['mean_motion', 'eccentricity', 'inclination', 'arg_perigee', 'raan', 'mean_anomaly']
            available_core_elements = [col for col in core_elements if col in processed_data.columns]
            print(f"   -> æ¨¡å‹å°†ä½¿ç”¨ä»¥ä¸‹åˆ—è¿›è¡Œæ»¤æ³¢: {available_core_elements}")
            pf_input_data = processed_data[available_core_elements]

        # 3. æ•°æ®åˆ’åˆ†
        train_data, test_data = self._split_data(pf_input_data)
        
        # 4. ã€æœ‰ç›‘ç£ã€‘è®­ç»ƒ
        self._train_detector(train_data, maneuver_times)
        
        # 5. åœ¨æµ‹è¯•é›†ä¸Šæ£€æµ‹
        detected_anomalies, anomaly_scores = self._detect_anomalies(test_data)
        
        # 6. è¯„ä¼°ç»“æœ
        metrics = self._evaluate(detected_anomalies, test_data, maneuver_times)
        
        # 7. å¯è§†åŒ–å’ŒæŠ¥å‘Š
        self._visualize_and_report(processed_data, test_data, detected_anomalies, anomaly_scores, maneuver_times, metrics)
        
        print("\nâœ… å®éªŒå®Œæˆï¼")
        return self.results
        
    def _load_data(self):
        """åŠ è½½TLEæ•°æ®å’ŒæœºåŠ¨æ—¥å¿—"""
        print("\n[1/7] ğŸ“‚ åŠ è½½æ•°æ®...")
        tle_data, maneuver_times = self.loader.load_satellite_data(self.config['satellite_name'])
    
        # éªŒè¯æ•°æ®
        print(f"   -> TLEæ•°æ®å½¢çŠ¶: {tle_data.shape}")
        print(f"   -> TLEæ•°æ®åˆ—: {list(tle_data.columns[:10])}...")  # æ˜¾ç¤ºå‰10åˆ—
    
        # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['mean_motion', 'eccentricity', 'inclination', 'mean_anomaly']
        missing_cols = [col for col in required_cols if col not in tle_data.columns]
        if missing_cols:
            print(f"   -> âš ï¸ è­¦å‘Šï¼šç¼ºå°‘å…³é”®åˆ—: {missing_cols}")
    
        print(f"   -> åŠ è½½äº† {len(tle_data)} æ¡TLEè®°å½•")
        print(f"   -> åŠ è½½äº† {len(maneuver_times)} ä¸ªçœŸå®æœºåŠ¨äº‹ä»¶")
    
        # æ˜¾ç¤ºæ—¶é—´èŒƒå›´
        if not tle_data.empty:
            print(f"   -> TLEæ—¶é—´èŒƒå›´: {tle_data['epoch'].min()} åˆ° {tle_data['epoch'].max()}")
        if maneuver_times:
            print(f"   -> æœºåŠ¨æ—¶é—´èŒƒå›´: {min(maneuver_times)} åˆ° {max(maneuver_times)}")
    
        self.results['total_tle_records'] = len(tle_data)
        self.results['total_maneuvers'] = len(maneuver_times)
        return tle_data, maneuver_times

    def _process_data(self, tle_data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨MeanElementsProcessorå¤„ç†æ•°æ®ï¼Œä¸ºç²’å­æ»¤æ³¢å™¨å‡†å¤‡è¾“å…¥"""
        print("\n[2/7] ğŸ› ï¸  å¤„ç†å¹³æ ¹æ•°æ•°æ®...")
        processed_data = self.processor.process_tle_data(tle_data, self.config['satellite_name'])
        print(self.processor.get_processing_report())
        
        # æ£€æŸ¥æ¯åˆ—çš„NaNæƒ…å†µ
        print("\n   -> æ£€æŸ¥æ•°æ®è´¨é‡:")
        for col in processed_data.columns:
            nan_count = processed_data[col].isna().sum()
            nan_percent = (nan_count / len(processed_data)) * 100 if len(processed_data) > 0 else 0
            if nan_percent > 0:
                print(f"      - {col}: {nan_count} NaNå€¼ ({nan_percent:.1f}%)")
    
        # å¦‚æœeccentricityå…¨æ˜¯NaNï¼Œå°è¯•ä»åŸå§‹æ•°æ®æ¢å¤
        if 'eccentricity' in processed_data.columns and processed_data['eccentricity'].isna().all():
            print("   -> âš ï¸ eccentricityåˆ—å…¨æ˜¯NaNï¼Œå°è¯•ä»åŸå§‹æ•°æ®æ¢å¤...")
            if 'eccentricity' in tle_data.columns:
                # Ensure the length matches for direct assignment
                if len(tle_data) >= len(processed_data):
                     processed_data['eccentricity'] = tle_data['eccentricity'].values[:len(processed_data)]
                     print(f"      - æ¢å¤åçš„eccentricityèŒƒå›´: [{processed_data['eccentricity'].min():.6f}, {processed_data['eccentricity'].max():.6f}]")
                else:
                    print("     -> æ¢å¤å¤±è´¥ï¼šåŸå§‹TLEæ•°æ®é•¿åº¦å°äºå¤„ç†åæ•°æ®")

        # ç§»é™¤å…¨æ˜¯NaNçš„åˆ—
        cols_before = len(processed_data.columns)
        processed_data = processed_data.dropna(axis=1, how='all')
        cols_after = len(processed_data.columns)
        if cols_before > cols_after:
            print(f"   -> ç§»é™¤äº† {cols_before - cols_after} ä¸ªå…¨æ˜¯NaNçš„åˆ—")
    
        # å¡«å……å‰©ä½™çš„NaNå€¼
        print("   -> æ­£åœ¨å¡«å……æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿçš„NaNå€¼...")
        processed_data.ffill(inplace=True)
        processed_data.bfill(inplace=True)
    
        if processed_data.isnull().values.any():
            warnings.warn("æ•°æ®å¡«å……åä»å­˜åœ¨NaNå€¼ï¼Œå°†ä½¿ç”¨0å¡«å……å‰©ä½™ç©ºå€¼ã€‚")
            processed_data.fillna(0, inplace=True)
    
        print(f"   -> å¤„ç†åå¾—åˆ° {len(processed_data)} æ¡æœ‰æ•ˆæ•°æ®")
        print(f"   -> æœ€ç»ˆä½¿ç”¨çš„åˆ—: {list(processed_data.columns)}")
    
        return processed_data

    def _split_data(self, data: pd.DataFrame) -> tuple:
        """æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        print("\n[3/7] ğŸ“Š åˆ’åˆ†æ•°æ®é›†...")
        split_ratio = self.config['train_split_ratio']
        split_index = int(len(data) * split_ratio)
        
        train_data = data.iloc[:split_index]
        test_data = data.iloc[split_index:]
        
        print(f"   -> è®­ç»ƒé›†: {len(train_data)} æ¡ ({split_ratio:.0%})")
        print(f"   -> æµ‹è¯•é›†: {len(test_data)} æ¡ ({1-split_ratio:.0%})")
        return train_data, test_data
    
    def _detect_anomalies(self, test_data: pd.DataFrame) -> tuple:
        """åœ¨æµ‹è¯•æ•°æ®ä¸Šæ‰§è¡Œå¼‚å¸¸æ£€æµ‹"""
        print("\n[5/7] ğŸ” åœ¨æµ‹è¯•é›†ä¸Šæ£€æµ‹æœºåŠ¨...")
        detected_anomalies, anomaly_scores = self.detector.detect_anomalies(test_data)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if len(anomaly_scores) > 0:
            print(f"   -> å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡:")
            print(f"      - æœ€å°å€¼: {np.min(anomaly_scores):.2f}")
            print(f"      - æœ€å¤§å€¼: {np.max(anomaly_scores):.2f}")
            print(f"      - å¹³å‡å€¼: {np.mean(anomaly_scores):.2f}")
            print(f"      - æ ‡å‡†å·®: {np.std(anomaly_scores):.2f}")
        print(f"      - é˜ˆå€¼: {self.detector.anomaly_threshold:.2f}")
    
        # æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„æ—¶é—´ç‚¹
        if len(anomaly_scores) > 10:
            top_scores_idx = np.argsort(anomaly_scores)[-10:]
            print(f"   -> å¾—åˆ†æœ€é«˜çš„10ä¸ªæ—¶é—´ç‚¹:")
            # score_index starts from the second element of test_data
            score_index_map = test_data.index[1:]
            for idx in top_scores_idx:
                if idx < len(score_index_map):
                    timestamp = score_index_map[idx]
                    score = anomaly_scores[idx]
                    print(f"      {timestamp}: {score:.2f}")        
        
        print(f"   -> æ£€æµ‹åˆ° {len(detected_anomalies)} ä¸ªæ½œåœ¨æœºåŠ¨äº‹ä»¶ã€‚")
        self.results['detected_anomalies_count'] = len(detected_anomalies)
        return detected_anomalies, anomaly_scores
    
    def _evaluate(self, detected_anomalies: list, test_data: pd.DataFrame, maneuver_times: list) -> dict:
        """è¯„ä¼°æ£€æµ‹æ€§èƒ½"""
        print("\n[6/7] ğŸ“ˆ è¯„ä¼°æ£€æµ‹æ€§èƒ½...")
    
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæµ‹è¯•é›†çš„æ—¶é—´èŒƒå›´
        print(f"   -> æµ‹è¯•é›†æ—¶é—´èŒƒå›´: {test_data.index.min()} åˆ° {test_data.index.max()}")
    
        # æ˜¾ç¤ºæµ‹è¯•é›†ä¸­çš„çœŸå®æœºåŠ¨
        true_maneuvers_in_test = []
        window = timedelta(days=self.config.get('label_window_days', 1))
    
        for m_time in maneuver_times:
            if test_data.index.min() <= m_time <= test_data.index.max():
                true_maneuvers_in_test.append(m_time)
    
        print(f"   -> æµ‹è¯•é›†ä¸­çš„çœŸå®æœºåŠ¨æ—¶é—´ (å‰5ä¸ª):")
        if not true_maneuvers_in_test:
            print("      æ— ")
        for i, m_time in enumerate(true_maneuvers_in_test[:5]):
            print(f"      {i+1}. {m_time}")
    
        # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„å¼‚å¸¸
        if detected_anomalies:
            print(f"   -> æ£€æµ‹åˆ°çš„å¼‚å¸¸æ—¶é—´ (å‰5ä¸ª):")
            for i, d_time in enumerate(detected_anomalies[:5]):
                print(f"      {i+1}. {d_time}")
                # æ£€æŸ¥æœ€è¿‘çš„çœŸå®æœºåŠ¨
                if true_maneuvers_in_test:
                    closest_maneuver = min(true_maneuvers_in_test,
                                          key=lambda x: abs((x - d_time).total_seconds()))
                    distance_days = abs((closest_maneuver - d_time).total_seconds()) / 86400
                    print(f"         æœ€è¿‘çš„çœŸå®æœºåŠ¨è·ç¦»: {distance_days:.1f} å¤©")
        else:
            print("   -> æœªæ£€æµ‹åˆ°ä»»ä½•å¼‚å¸¸ã€‚")
            
        # åˆ›å»ºçœŸå®æ ‡ç­¾
        true_labels = pd.Series(0, index=test_data.index)
        true_maneuver_in_test_count = 0
    
        for m_time in maneuver_times:
            if test_data.index.min() <= m_time <= test_data.index.max():
                true_maneuver_in_test_count += 1
                mask = (true_labels.index >= m_time - window) & (true_labels.index <= m_time + window)
                true_labels[mask] = 1
    
        # åˆ›å»ºé¢„æµ‹æ ‡ç­¾
        pred_labels = pd.Series(0, index=test_data.index)
        if detected_anomalies:
            valid_anomalies = [ts for ts in detected_anomalies if ts in pred_labels.index]
            if valid_anomalies:
                pred_labels.loc[valid_anomalies] = 1
    
        # è®¡ç®—æ··æ·†çŸ©é˜µå…ƒç´ 
        tp = ((pred_labels == 1) & (true_labels == 1)).sum()
        fp = ((pred_labels == 1) & (true_labels == 0)).sum()
        fn = ((pred_labels == 0) & (true_labels == 1)).sum()
        tn = ((pred_labels == 0) & (true_labels == 0)).sum()
    
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
        metrics = {
            'TP': int(tp),
            'FP': int(fp),
            'FN': int(fn),
            'TN': int(tn),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1)
        }
    
        print(f"   -> æµ‹è¯•é›†ä¸­çš„çœŸå®æœºåŠ¨äº‹ä»¶æ•°: {true_maneuver_in_test_count}")
        print(f"   -> å¸¦æ ‡ç­¾çš„æ—¶é—´ç‚¹æ•° (çª—å£={window.days*2}å¤©): {(true_labels == 1).sum()}")
        print(f"   -> æ€§èƒ½æŒ‡æ ‡:")
        print(f"      - ç²¾ç¡®ç‡ (Precision): {precision:.3f}")
        print(f"      - å¬å›ç‡ (Recall):    {recall:.3f}")
        print(f"      - F1åˆ†æ•° (F1-Score):   {f1:.3f}")
        print(f"      - TP: {tp}, FP: {fp}, FN: {fn}")
    
        self.results['performance_metrics'] = metrics
        return metrics

    def _visualize_and_report(self, all_data, test_data, anomalies, scores, maneuver_times, metrics):
        """ç”Ÿæˆå¹¶ä¿å­˜å›¾è¡¨å’ŒæŠ¥å‘Š"""
        print("\n[7/7] ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’ŒæŠ¥å‘Š...")

        # --- å¯è§†åŒ– ---
        fig, axes = plt.subplots(2, 1, figsize=(18, 12), sharex=True)
        fig.suptitle(f"Particle Filter Maneuver Detection for {self.config['satellite_name']}", fontsize=16)

        # å›¾1: Mean Motion å’Œæ£€æµ‹ç»“æœ
        ax1 = axes[0]
        ax1.plot(all_data.index, all_data['mean_motion'], color='gray', alpha=0.5, label='Mean Motion (All Data)')
        ax1.plot(test_data.index, test_data['mean_motion'], 'b-', label='Mean Motion (Test Set)')
        
        # æ ‡è®°çœŸå®æœºåŠ¨
        true_man_in_test = [m for m in maneuver_times if test_data.index.min() <= m <= test_data.index.max()]
        if true_man_in_test:
             ax1.vlines(true_man_in_test, ymin=ax1.get_ylim()[0], ymax=ax1.get_ylim()[1], color='green', linestyle='--', label='True Maneuver', lw=2)
        
        # æ ‡è®°æ£€æµ‹åˆ°çš„æœºåŠ¨
        if anomalies:
            valid_anomalies = [a for a in anomalies if a in test_data.index]
            if valid_anomalies:
                anomaly_values = test_data.loc[valid_anomalies]['mean_motion']
                ax1.scatter(valid_anomalies, anomaly_values, color='red', s=80, marker='^', label='Detected Maneuver', zorder=5)
        
        ax1.set_ylabel("Mean Motion (rev/day)")
        ax1.set_title("Maneuver Detection Results")
        ax1.legend()
        ax1.grid(True, which='both', linestyle='--', linewidth=0.5)

        # å›¾2: å¼‚å¸¸åˆ†æ•°å’Œé˜ˆå€¼
        ax2 = axes[1]
        # å¼‚å¸¸åˆ†æ•°æ˜¯ä»ç¬¬äºŒä¸ªæ—¶é—´ç‚¹å¼€å§‹çš„ï¼Œæ‰€ä»¥éœ€è¦å¯¹é½
        if len(scores) > 0:
            score_index = test_data.index[1:len(scores)+1]
            ax2.plot(score_index, scores, 'r-', alpha=0.8, label='Anomaly Score')
            ax2.axhline(y=self.detector.anomaly_threshold, color='black', linestyle=':', label=f"Threshold ({self.detector.anomaly_threshold:.2f})", lw=2)
            ax2.set_yscale('log') # ä½¿ç”¨å¯¹æ•°åæ ‡è½´ä»¥ä¾¿è§‚å¯Ÿ

        
        ax2.set_xlabel("Time")
        ax2.set_ylabel("Anomaly Score (Negative Log-Likelihood)")
        ax2.set_title("Anomaly Scores Over Time")
        ax2.legend()
        ax2.grid(True, which='both', linestyle='--', linewidth=0.5)
        
        fig_path = os.path.join(self.config['output_dir'], f"{self.config['satellite_name']}_pf_detection_results.png")
        plt.savefig(fig_path, dpi=300)
        plt.close(fig)
        print(f"   -> å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {fig_path}")

        # --- ç”ŸæˆæŠ¥å‘Š ---
        report = {
            "experiment_config": self.config,
            "data_summary": {
                "total_tle_records": self.results.get('total_tle_records'),
                "total_maneuvers": self.results.get('total_maneuvers'),
                "processed_records": len(all_data),
                "train_set_size": len(test_data.iloc[:int(len(test_data)*self.config['train_split_ratio'])])
            },
            "model_summary": self.detector.get_model_summary(),
            "performance_metrics": metrics,
            "detected_anomaly_timestamps": [ts.isoformat() for ts in anomalies]
        }
        
        report_path = os.path.join(self.config['output_dir'], f"{self.config['satellite_name']}_pf_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=4)
        
        print(f"   -> è¯¦ç»†JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

# =====================================================================
# ä¸»æ‰§è¡Œå‡½æ•°
# =====================================================================
def main():
    """ä¸»å‡½æ•°ï¼Œé…ç½®å¹¶è¿è¡Œå®éªŒ"""
    
    experiment_config = {
        'satellite_name': 'Fengyun-4A',
        'data_dir': 'data',
        'output_dir': 'outputs/particle_filter_supervised_test',
        
        'train_split_ratio': 0.7,
        
        # ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰æ‰¾åˆ°çš„æœ€ä¼˜é…ç½®
        'n_particles': 1000,
        'use_only_mean_motion': True,
        'noise_scaling_factor': 0.2,

        # è¯„ä¼°ç”¨çš„æ—¶é—´çª—å£
        'label_window_days': 3.0,       # ç”¨äºåœ¨è®­ç»ƒæ—¶å‰”é™¤æœºåŠ¨æ•°æ®
        'label_window_days_eval': 1.5,  # ç”¨äºè¯„ä¼°æ—¶æ ‡è®°çœŸå€¼
    }

    # è¿è¡Œå®éªŒ
    experiment = ParticleFilterSupervisedExperiment(experiment_config)
    experiment.run()


if __name__ == "__main__":
    main()