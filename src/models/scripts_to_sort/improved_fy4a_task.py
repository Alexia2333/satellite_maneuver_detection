import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

print("ğŸš€ æ”¹è¿›çš„é£äº‘4AæœºåŠ¨æ£€æµ‹")
print("=" * 50)


def group_and_filter_anomalies(timestamps, max_gap=pd.Timedelta(days=1), min_group_size=2): # <--- å¢åŠ  min_group_size å‚æ•°
    if timestamps.empty:
        return []

    timestamps = pd.Series(sorted(timestamps))
    groups = []
    current_group = [timestamps.iloc[0]]

    for i in range(1, len(timestamps)):
        if (timestamps.iloc[i] - timestamps.iloc[i-1]) <= max_gap:
            current_group.append(timestamps.iloc[i])
        else:
            if len(current_group) >= min_group_size:
                groups.extend(current_group)
            current_group = [timestamps.iloc[i]]

    if len(current_group) >= min_group_size:
        groups.extend(current_group)

    return sorted(list(set(groups)))
# =====================================================================
# å¢å¼ºçš„ç‰¹å¾å·¥ç¨‹ï¼ˆåŸºäºåŸæœ‰create_enhanced_featuresæ”¹è¿›ï¼‰
# =====================================================================
def create_drift_enhanced_features(tle_data, target_col='mean_motion'):
    print("\nğŸ› ï¸ åˆ›å»ºæ¼‚ç§»å¢å¼ºç‰¹å¾")
    print("-" * 20)
    
    data = tle_data.set_index('epoch').copy()
    base_cols = ['mean_motion', 'eccentricity', 'inclination']
    available_cols = [col for col in base_cols if col in data.columns]
    
    if not available_cols:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åŸºç¡€ç‰¹å¾åˆ—: {base_cols}")
        return None
    
    print(f"   åŸºç¡€å‚æ•°: {available_cols}")
    enhanced_data = data[available_cols].copy()
    
    lags = [1, 2, 3, 7]
    windows = [3, 7, 14]
    for col in available_cols:
        for lag in lags:
            enhanced_data[f'{col}_lag_{lag}'] = data[col].shift(lag)
        for window in windows:
            enhanced_data[f'{col}_rolling_mean_{window}'] = data[col].rolling(window, min_periods=window//2).mean()
            enhanced_data[f'{col}_rolling_std_{window}'] = data[col].rolling(window, min_periods=window//2).std()
        enhanced_data[f'{col}_diff_1'] = data[col].diff(1)
        enhanced_data[f'{col}_diff_7'] = data[col].diff(7)

    long_windows = [30, 60]
    for col in available_cols:
        for window in long_windows:
            long_mean = data[col].rolling(window, min_periods=window//3).mean()
            long_std = data[col].rolling(window, min_periods=window//3).std()
            enhanced_data[f'{col}_drift_from_{window}d'] = data[col] - long_mean
            enhanced_data[f'{col}_drift_zscore_{window}d'] = (data[col] - long_mean) / (long_std + 1e-8)

    trend_windows = [7, 14]
    for col in available_cols:
        for window in trend_windows:
            enhanced_data[f'{col}_trend_{window}d'] = data[col].rolling(window).apply(
                lambda x: (x[-1] - x[0]) / len(x) if len(x) > 1 else 0, raw=True)

    for col in available_cols:
        short_mean = data[col].rolling(7, min_periods=3).mean()
        short_drift = (data[col] - short_mean).abs()
        enhanced_data[f'{col}_cumulative_drift_7d'] = short_drift.rolling(7, min_periods=3).sum()
        enhanced_data[f'{col}_cumulative_drift_14d'] = short_drift.rolling(14, min_periods=7).sum()

    if len(available_cols) > 1:
        for window in [7, 14]:
            drift_cols = [f'{col}_drift_from_30d' for col in available_cols if f'{col}_drift_from_30d' in enhanced_data.columns]
            if drift_cols:
                enhanced_data[f'combined_drift_l2_{window}d'] = np.sqrt(enhanced_data[drift_cols].pow(2).sum(axis=1))

    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆå·®åˆ†ï¼‰
    mean_motion_diff = data[target_col].diff()
    enhanced_data['target'] = mean_motion_diff.shift(-1).abs()

    # âœ… æ— éœ€ç­›é€‰å°å€¼ï¼šä¿ç•™æ‰€æœ‰éNaNï¼Œç›´æ¥åšæ”¾å¤§å¤„ç†ï¼ˆé‡ç‚¹ï¼ï¼ï¼‰
    enhanced_data = enhanced_data.dropna(subset=['target'])

    # âœ… æ•°å€¼æ”¾å¤§ + log1pï¼Œæå‡æ•°å€¼å¯å­¦ä¹ æ€§
    enhanced_data['target'] = np.log1p(enhanced_data['target'] * 1e8)

    # æ‰“å°ç¡®è®¤
    print("âœ… åº”ç”¨äº† log1p(target * 1e8) å˜æ¢")
    print(enhanced_data['target'].describe())
    


    # å¡«å……å…¶ä»–ç‰¹å¾
    enhanced_data = enhanced_data.fillna(method='ffill', limit=3)
    enhanced_data = enhanced_data.fillna(method='bfill', limit=3)
    for col in enhanced_data.columns:
        if enhanced_data[col].isna().any():
            median_val = enhanced_data[col].median()
            enhanced_data[col].fillna(median_val if not pd.isna(median_val) else 0, inplace=True)

    print(f"âœ… ç‰¹å¾åˆ›å»ºå®Œæˆ")
    print(f"   ç‰¹å¾æ•°é‡: {len(enhanced_data.columns)}")
    print(f"   æœ‰æ•ˆè®°å½•: {len(enhanced_data)}")
    print(f"   ç›®æ ‡å˜é‡ç»Ÿè®¡: mean={enhanced_data['target'].mean():.6f}, std={enhanced_data['target'].std():.6f}")
    return enhanced_data


# =====================================================================
# ä½¿ç”¨æ”¹è¿›çš„æ£€æµ‹å™¨
# =====================================================================
def run_improved_detection(enhanced_data, maneuver_times):
    """è¿è¡Œæ”¹è¿›çš„æ£€æµ‹ - ä½¿ç”¨ç®€åŒ–æ–¹æ³•"""
    print("\nğŸ¤– è¿è¡Œæ”¹è¿›çš„æ¼‚ç§»æ„ŸçŸ¥æ£€æµ‹")
    print("-" * 30)
    
    try:
        from models.hybrid.xgboost_detector import create_labels_for_split
        import xgboost as xgb
        
        # 1. æ•°æ®åˆ’åˆ†
        split_ratio = 0.7
        split_index = int(len(enhanced_data) * split_ratio)
        train_data = enhanced_data.iloc[:split_index]
        test_data = enhanced_data.iloc[split_index:]
        
        print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡")
        print(f"   æµ‹è¯•é›†: {len(test_data)} æ¡")
        
        # 2. æå–æ­£å¸¸æ•°æ®è®­ç»ƒ
        train_labels = create_labels_for_split(
            train_data.index,
            maneuver_times,
            window=timedelta(days=1)
        )
        
        # ä½¿ç”¨æ­£å¸¸æ•°æ®è®­ç»ƒ
        normal_mask = (train_labels == 0)
        normal_train_data = train_data[normal_mask]
        print(f"   æ­£å¸¸è®­ç»ƒæ•°æ®: {len(normal_train_data)} æ¡")
        
        # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = normal_train_data.drop(columns=['target'])
        y_train = normal_train_data['target']
        feature_names = list(X_train.columns)
        
        print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"   ç›®æ ‡ç»Ÿè®¡: mean={y_train.mean():.6f}, std={y_train.std():.6f}, max={y_train.max():.6f}")
        
        # 4. è®­ç»ƒXGBoostæ¨¡å‹
        print("   ğŸ”§ è®­ç»ƒXGBoostæ¨¡å‹...")
        model = xgb.XGBRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            verbosity=0
        )
        
        model.fit(X_train, y_train)
        
        # 5. è®¡ç®—è®­ç»ƒé›†æ®‹å·®å’Œé˜ˆå€¼
        print("   ğŸ“Š è®¡ç®—è®­ç»ƒé›†æ€§èƒ½å’Œé˜ˆå€¼...")
        y_train_pred = model.predict(X_train)
        train_residuals = np.abs(y_train - y_train_pred)
        
        epsilon = 1e-8 
        train_change_rates = np.abs(y_train - y_train_pred) / (np.abs(y_train) + epsilon)

        threshold_percentile = 99.5
        residual_threshold = train_residuals.mean() + 3 * train_residuals.std()
        change_rate_threshold_val = np.percentile(train_change_rates, 99.5)
        
        print(f"   è®­ç»ƒRÂ²: {model.score(X_train, y_train):.4f}")
        print(f"   æ®‹å·®é˜ˆå€¼ ({threshold_percentile}%): {residual_threshold:.6f}")
        print(f"   å˜åŒ–ç‡é˜ˆå€¼ ({threshold_percentile}%): {change_rate_threshold_val:.6f}")
        
        # 6. åœ¨æµ‹è¯•é›†ä¸Šæ£€æµ‹
        print("   ğŸ” æ£€æµ‹å¼‚å¸¸...")
        X_test = test_data[feature_names]
        y_test = test_data['target']
        
        y_test_log_pred = model.predict(X_test)
        y_test_pred = y_test_log_pred
        y_test_actual = y_test
        test_residuals = np.abs(y_test_actual - y_test_pred)
        # åŸºç¡€æ£€æµ‹
        anomaly_mask = test_residuals > residual_threshold
        anomaly_indices = np.where(anomaly_mask)[0]
        
        print(f"   åŸºç¡€æ£€æµ‹: {len(anomaly_indices)} ä¸ªå¼‚å¸¸")
        
        # 7. ç»“åˆæ¼‚ç§»å‹åŠ›çš„å¢å¼ºæ£€æµ‹
        print("   ğŸ§ª åº”ç”¨ç»“åˆæ¼‚ç§»å‹åŠ›çš„å¢å¼ºæ£€æµ‹...")

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        X_test = test_data[feature_names]
        y_test = test_data['target']
        y_test_pred = model.predict(X_test)

        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        test_residuals = np.abs(y_test - y_test_pred)
        test_change_rates = np.abs(y_test - y_test_pred) / (np.abs(y_test) + epsilon)
        
        # å®šä¹‰ä¸¤ä¸ªæ£€æµ‹è·¯å¾„ï¼š
        # è·¯å¾„A: "å¼ºä¿¡å·" - æ— è®ºå‹åŠ›å¦‚ä½•ï¼Œåªè¦æ®‹å·®å’Œå˜åŒ–ç‡éƒ½æé«˜ï¼Œå°±è®¤ä¸ºæ˜¯å¼‚å¸¸
        strong_signal_mask = (test_residuals > residual_threshold) & (test_change_rates > change_rate_threshold_val)
        print(f"      - å¼ºä¿¡å·æ£€æµ‹ï¼ˆANDé€»è¾‘ï¼‰å‘ç°: {strong_signal_mask.sum()} ä¸ªå¼‚å¸¸")

        # è·¯å¾„B: "å‹åŠ›è¾…åŠ©ä¿¡å·" - åœ¨è½¨é“æ¼‚ç§»å‹åŠ›å¤§çš„æ—¶æœŸï¼Œé€‚å½“æ”¾å®½æ ‡å‡†
        # åˆå§‹åŒ–ä¸€ä¸ªå…¨ä¸ºFalseçš„mask
        pressure_assisted_mask = pd.Series(False, index=test_data.index)
        
        # è¿™éƒ¨åˆ†ä»£ç ä¼šå®šä¹‰åç»­å¯è§†åŒ–å’ŒæŠ¥å‘Šæ‰€éœ€çš„å˜é‡
        drift_features = [col for col in feature_names if any(keyword in col for keyword in ['drift', 'cumulative'])]
        if drift_features:
            print("      - è®¡ç®—æ¼‚ç§»å‹åŠ›ä»¥è¾…åŠ©æ£€æµ‹...")
            # è®¡ç®—æ¼‚ç§»å‹åŠ›
            train_drift_pressure = train_data[drift_features].abs().mean(axis=1)
            test_drift_pressure = test_data[drift_features].abs().mean(axis=1)

            # ä½¿ç”¨è®­ç»ƒé›†çš„ä¸­ä½æ•°ä½œä¸ºå‹åŠ›åŸºçº¿ï¼Œæ›´ç¨³å¥
            pressure_baseline = np.median(train_drift_pressure)
            
            # å¯¹æµ‹è¯•é›†çš„å‹åŠ›è¿›è¡Œæ ‡å‡†åŒ–
            # ä¸ºé¿å…é™¤ä»¥0ï¼Œå¦‚æœåŸºçº¿ä¸º0åˆ™ä¸è¿›è¡Œæ ‡å‡†åŒ–
            if pressure_baseline > 0:
                normalized_pressure = test_drift_pressure / pressure_baseline
            else:
                normalized_pressure = test_drift_pressure
            
            # éå†æ¯ä¸€ä¸ªç‚¹ï¼Œåº”ç”¨å‹åŠ›è°ƒæ•´é€»è¾‘
            for i in range(len(test_residuals)):
                # åªåœ¨å‹åŠ›å¤§äºåŸºçº¿æ—¶ï¼Œæ‰è€ƒè™‘é™ä½é˜ˆå€¼
                if normalized_pressure.iloc[i] > 1.0: 
                    # å‹åŠ›è¶Šå¤§ï¼Œé˜ˆå€¼é™ä½å¾—è¶Šå¤šï¼Œä½†æœ€å¤šé™ä½30%
                    reduction_factor = 1.0 - 0.3 * min((normalized_pressure.iloc[i] - 1.0), 1.0)
                    adjusted_threshold = residual_threshold * reduction_factor
                    
                    # å¦‚æœç‚¹çš„æ®‹å·®è¶…è¿‡äº†ã€è°ƒæ•´åã€‘çš„é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºå¼‚å¸¸
                    if test_residuals[i] > adjusted_threshold:
                        pressure_assisted_mask.iloc[i] = True
            
            print(f"      - å‹åŠ›è¾…åŠ©æ£€æµ‹å‘ç°: {pressure_assisted_mask.sum()} ä¸ªå¼‚å¸¸")
        
        # åˆå¹¶ä¸¤ä¸ªè·¯å¾„çš„ç»“æœï¼šæ»¡è¶³ä»»ä½•ä¸€ä¸ªè·¯å¾„çš„éƒ½æ˜¯å¼‚å¸¸
        final_anomaly_mask = strong_signal_mask | pressure_assisted_mask
        initial_anomaly_indices = np.where(final_anomaly_mask)[0]

        # ä¸ºåç»­æ­¥éª¤ï¼ˆå¦‚å›¾è¡¨å’ŒæŠ¥å‘Šï¼‰ä¿å­˜å¿…è¦çš„å˜é‡
        drift_pressure_test = test_drift_pressure if 'test_drift_pressure' in locals() else None

        # 8. æ—¶åºèšç±»ä¸è¿‡æ»¤
        initial_anomaly_timestamps = test_data.index[initial_anomaly_indices]
        
        if not initial_anomaly_timestamps.empty:
            print("   åº”ç”¨æ—¶åºèšç±»å’Œè¿‡æ»¤...")
            clustered_timestamps = group_and_filter_anomalies(
                initial_anomaly_timestamps, 
                max_gap=pd.Timedelta(days=1.5), 
                min_group_size=3  # è¦æ±‚ä¸€ä¸ªæœºåŠ¨äº‹ä»¶è‡³å°‘äº§ç”Ÿ2ä¸ªè¿ç»­çš„å¼‚å¸¸ç‚¹
            )
            
            # ç”¨è¿‡æ»¤åçš„ç»“æœä½œä¸ºæœ€ç»ˆçš„å¼‚å¸¸æ—¶é—´æˆ³
            anomaly_timestamps = clustered_timestamps
            print(f"   èšç±»å’Œè¿‡æ»¤åï¼Œæœ€ç»ˆå¼‚å¸¸äº‹ä»¶: {len(anomaly_timestamps)} ä¸ª")
        else:
            anomaly_timestamps = pd.DatetimeIndex([]) # å¦‚æœæ²¡æœ‰åˆå§‹å¼‚å¸¸ç‚¹ï¼Œåˆ™ç»“æœä¸ºç©º
            print("   æ²¡æœ‰å‘ç°ç¬¦åˆæ¡ä»¶çš„å¼‚å¸¸äº‹ä»¶ã€‚")

        # ä¸ºäº†åç»­ä»£ç å…¼å®¹ï¼Œè·å–æœ€ç»ˆçš„ç´¢å¼•
        anomaly_indices = [test_data.index.get_loc(ts) for ts in anomaly_timestamps if ts in test_data.index]
        
        drift_analysis = {}
        class SimpleDetector:
            def __init__(self, model, threshold, features):
                self.model = model
                self.residual_threshold = threshold
                self.feature_names = features
        
        
        detector = SimpleDetector(model, residual_threshold, feature_names)        
        # 9. æ¼‚ç§»åˆ†æ
        
        if drift_features and drift_pressure_test is not None:
            # ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–æ–¹æ³•
            if pressure_baseline > 0:
                drift_pressure_normalized = drift_pressure_test / pressure_baseline
            else:
                drift_pressure_normalized = drift_pressure_test
                
            drift_analysis = {
                'mean_pressure': float(drift_pressure_normalized.mean()),
                'max_pressure': float(drift_pressure_normalized.max()),
                'high_pressure_ratio': float((drift_pressure_normalized > 0.7).mean()),
                'pressure_trend': float(drift_pressure_normalized.iloc[-7:].mean() - drift_pressure_normalized.iloc[:7].mean()),
                'high_pressure_periods': []
            }
            
            # æ‰¾é«˜å‹åŠ›æœŸé—´
            high_pressure_mask = drift_pressure_normalized > 0.7
            if high_pressure_mask.any():
                changes = high_pressure_mask.astype(int).diff()
                starts = test_data.index[changes == 1]
                ends = test_data.index[changes == -1]
                
                if high_pressure_mask.iloc[0]:
                    starts = pd.Index([test_data.index[0]]).append(starts)
                if high_pressure_mask.iloc[-1]:
                    ends = ends.append(pd.Index([test_data.index[-1]]))
                    
                for start, end in zip(starts[:5], ends[:5]):  # åªå–å‰5ä¸ª
                    period_mask = (test_data.index >= start) & (test_data.index <= end)
                    drift_analysis['high_pressure_periods'].append({
                        'start': start,
                        'end': end,
                        'duration_days': (end - start).days,
                        'max_pressure': float(drift_pressure_normalized[period_mask].max())
                    })
        
        print(f"\nâœ… æ£€æµ‹ç»“æœ:")
        print(f"   æ£€æµ‹å¼‚å¸¸æ•°: {len(anomaly_timestamps)}")
        print(f"   å¼‚å¸¸æ¯”ä¾‹: {len(anomaly_timestamps)/len(test_data)*100:.2f}%")
        
        if drift_analysis:
            print(f"\nğŸ“Š æ¼‚ç§»åˆ†æ:")
            print(f"   å¹³å‡æ¼‚ç§»å‹åŠ›: {drift_analysis['mean_pressure']:.3f}")
            print(f"   æœ€å¤§æ¼‚ç§»å‹åŠ›: {drift_analysis['max_pressure']:.3f}")
            print(f"   é«˜å‹åŠ›æ—¶é—´æ¯”ä¾‹: {drift_analysis['high_pressure_ratio']*100:.1f}%")
        
        # è¿”å›ç®€åŒ–çš„ç»“æœ
        class SimpleDetector:
            def __init__(self, model, threshold, features, pressure_baseline=1.0):
                self.model = model
                self.residual_threshold = threshold
                self.feature_names = features
                self.pressure_threshold = pressure_baseline  
                self.pressure_baseline = pressure_baseline
                
            def _calculate_drift_pressure(self, data):
                drift_feats = [col for col in self.feature_names 
                              if any(k in col for k in ['drift', 'cumulative'])]
                if drift_feats and all(col in data.columns for col in drift_feats):
                    pressure = data[drift_feats].abs().mean(axis=1)
                    # æ ‡å‡†åŒ–
                    if self.pressure_baseline > 0:
                        return pressure / self.pressure_baseline
                    return pressure
                return pd.Series(0, index=data.index)
        
        detector = SimpleDetector(model, residual_threshold, feature_names, 
                                 pressure_baseline if 'pressure_baseline' in locals() else 1.0)
        

        print("ğŸ“Š y_train sample:", y_train.head())
        print("ğŸ“Š y_train max:", y_train.max())

        return anomaly_timestamps, (anomaly_indices, test_residuals), test_data, detector, drift_analysis
        
    except Exception as e:
        print(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, None
def detect_anomalies_with_enhancements(y_true, y_pred, pressure_series,
                                       residual_threshold=0.18,
                                       change_rate_threshold=0.35,
                                       rolling_window=7,
                                       pressure_alpha=1.0,
                                       baseline_pressure=0.7):
    """
    ç»¼åˆ A/B/C ç­–ç•¥è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼š
    A. ç›¸å¯¹å˜åŒ–ç‡
    B. æ»‘åŠ¨æ®‹å·®å¹³å‡
    C. å‹åŠ›åŠ æƒè°ƒæ•´
    """
    residual = np.abs(y_true - y_pred)
    change_rate = np.abs(y_true - y_pred) / (np.abs(y_true) + 1e-6)
    rolling_residual = pd.Series(residual).rolling(rolling_window, center=True, min_periods=1).mean()
    pressure_weight = 1 + pressure_alpha * np.exp(np.clip(pressure_series - baseline_pressure, 0, 5))
    adjusted_residual = rolling_residual * pressure_weight

    # ç»¼åˆæ£€æµ‹é€»è¾‘
    anomaly_mask = (
        (residual > residual_threshold) |
        (change_rate > change_rate_threshold) |
        (adjusted_residual > residual_threshold)
    )
    return anomaly_mask, {
        'residual': residual,
        'change_rate': change_rate,
        'rolling_residual': rolling_residual,
        'adjusted_residual': adjusted_residual
    }
# =====================================================================
# è¯„ä¼°å‡½æ•°ï¼ˆå¤ç”¨åŸæœ‰ä»£ç ï¼‰
# =====================================================================
def evaluate_detection(predictions, test_data, maneuver_times):
    """è¯„ä¼°æ£€æµ‹ç»“æœ"""
    print("\nğŸ“Š è¯„ä¼°æ£€æµ‹æ€§èƒ½")
    print("-" * 20)
    
    if predictions is None or len(predictions) == 0:
        print("âš ï¸ æ²¡æœ‰é¢„æµ‹ç»“æœ")
        return None
    
    from models.hybrid.xgboost_detector import create_labels_for_split
    from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
    
    # åˆ›å»ºæ ‡ç­¾
    true_labels = create_labels_for_split(
        test_data.index,
        maneuver_times,
        window=timedelta(days=1)
    )
    
    pred_labels = pd.Series(0, index=test_data.index)
    for timestamp in predictions:
        if timestamp in pred_labels.index:
            pred_labels.loc[timestamp] = 1
    
    # è®¡ç®—æŒ‡æ ‡
    precision = precision_score(true_labels, pred_labels, zero_division=0)
    recall = recall_score(true_labels, pred_labels, zero_division=0)
    f1 = f1_score(true_labels, pred_labels, zero_division=0)
    
    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()
    
    print(f"âœ… æ€§èƒ½æŒ‡æ ‡:")
    print(f"   ç²¾ç¡®ç‡: {precision:.3f}")
    print(f"   å¬å›ç‡: {recall:.3f}")
    print(f"   F1åˆ†æ•°: {f1:.3f}")
    print(f"   TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}")
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn
    }

# =====================================================================
# å¯è§†åŒ–ï¼ˆæ”¹è¿›ç‰ˆï¼‰
# =====================================================================
def visualize_drift_detection(enhanced_data, predictions, maneuver_times, 
                            drift_analysis, detector):
    """å¯è§†åŒ–æ¼‚ç§»å’Œæ£€æµ‹ç»“æœ - åŒ…å«é¢„æµ‹å€¼å¯¹æ¯”"""
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–...")
    
    # åˆ›å»ºä¸¤ä¸ªå›¾å½¢
    
    # å›¾1ï¼šæ¨¡å‹é¢„æµ‹å€¼ vs å®é™…å€¼
    fig1, axes1 = plt.subplots(2, 1, figsize=(15, 10))
    fig1.suptitle('é£äº‘4A - æ¨¡å‹é¢„æµ‹å€¼ vs å®é™…å€¼', fontsize=14)
    
    # å­å›¾1ï¼šç›®æ ‡å˜é‡çš„é¢„æµ‹å€¼å’Œå®é™…å€¼
    ax1 = axes1[0]
    if detector and hasattr(detector, 'model') and detector.model is not None:
        # è·å–æµ‹è¯•æ•°æ®çš„æœ€åéƒ¨åˆ†ç”¨äºå±•ç¤º
        display_data = enhanced_data.iloc[-200:]  # æœ€å200ä¸ªç‚¹
        
        # é¢„æµ‹
        X_display = display_data[detector.feature_names]
        y_pred_log = detector.model.predict(X_display)
        y_pred = y_pred_log
        y_true = display_data['target']
        residuals = np.abs(y_true - y_pred)

        # ç»˜åˆ¶
        ax1.plot(display_data.index, y_true, 'b-', alpha=0.7, linewidth=1.5, label='å®é™…å€¼')
        ax1.plot(display_data.index, y_pred, 'r--', alpha=0.7, linewidth=1.5, label='é¢„æµ‹å€¼')
        
        # æ ‡è®°å¤§çš„è·³å˜ç‚¹
        threshold = y_true.quantile(0.95)
        jumps = y_true > threshold
        ax1.scatter(display_data.index[jumps], y_true[jumps], 
                   color='orange', s=50, alpha=0.7, zorder=5, label='æ˜¾è‘—è·³å˜')
        
        ax1.set_ylabel('Mean Motion å˜åŒ–é‡(ç»å¯¹å€¼)')
        ax1.set_title('ç›®æ ‡å˜é‡ï¼šé¢„æµ‹å€¼ vs å®é™…å€¼')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šæ®‹å·®
    ax2 = axes1[1]
    if detector and hasattr(detector, 'model') and detector.model is not None:
        residuals = np.abs(y_true - y_pred)
        ax2.plot(display_data.index, residuals, 'g-', alpha=0.7, linewidth=1)
        
        # æ ‡è®°æ£€æµ‹é˜ˆå€¼
        if hasattr(detector, 'residual_threshold'):
            ax2.axhline(y=detector.residual_threshold, color='red', 
                       linestyle='--', alpha=0.7, label=f'æ£€æµ‹é˜ˆå€¼: {detector.residual_threshold:.6f}')
        
        ax2.set_ylabel('é¢„æµ‹æ®‹å·®(ç»å¯¹å€¼)')
        ax2.set_xlabel('æ—¶é—´')
        ax2.set_title('æ¨¡å‹æ®‹å·®')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾1
    output_dir = 'outputs/fy4a_improved'
    os.makedirs(output_dir, exist_ok=True)
    output_path1 = os.path.join(output_dir, 'model_predictions.png')
    plt.savefig(output_path1, dpi=300, bbox_inches='tight')
    print(f"   å·²ä¿å­˜åˆ°: {output_path1}")
    plt.close()
    
    # å›¾2ï¼šæœºåŠ¨äº‹ä»¶æ£€æµ‹å¯¹æ¯”
    fig2, axes2 = plt.subplots(3, 1, figsize=(15, 10))
    fig2.suptitle('é£äº‘4A - æœºåŠ¨äº‹ä»¶æ£€æµ‹å¯¹æ¯”', fontsize=14)
    
    # å­å›¾1ï¼šMean Motionå’Œäº‹ä»¶æ ‡è®°
    ax1 = axes2[0]
    ax1.plot(enhanced_data.index, enhanced_data['mean_motion'], 
             'b-', alpha=0.7, linewidth=1, label='Mean Motion')
    
    # æ ‡è®°çœŸå®æœºåŠ¨ï¼ˆçº¢è‰²è™šçº¿ï¼‰
    for i, m_time in enumerate(maneuver_times):
        if enhanced_data.index[0] <= m_time <= enhanced_data.index[-1]:
            ax1.axvline(m_time, color='red', alpha=0.6, linestyle='--',
                       label='çœŸå®æœºåŠ¨' if i == 0 else '')
    
    # æ ‡è®°é¢„æµ‹æœºåŠ¨ï¼ˆç»¿è‰²ç‚¹çº¿ï¼‰
    if predictions is not None and len(predictions) > 0:
        for i, pred_time in enumerate(predictions):
            ax1.axvline(pred_time, color='green', alpha=0.6, linestyle=':',
                       label='é¢„æµ‹æœºåŠ¨' if i == 0 else '')
    
    ax1.set_ylabel('Mean Motion')
    ax1.set_title('è½¨é“å‚æ•°ä¸æœºåŠ¨äº‹ä»¶')
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šçœŸå®æœºåŠ¨æ—¶é—´è½´
    ax2 = axes2[1]
    # åˆ›å»ºçœŸå®æœºåŠ¨çš„æ—¶é—´è½´
    maneuver_in_range = [m for m in maneuver_times 
                        if enhanced_data.index[0] <= m <= enhanced_data.index[-1]]
    if maneuver_in_range:
        y_pos = [1] * len(maneuver_in_range)
        ax2.scatter(maneuver_in_range, y_pos, color='red', s=100, marker='v', label='çœŸå®æœºåŠ¨')
        for i, m_time in enumerate(maneuver_in_range):
            ax2.text(m_time, 1.1, f'{i+1}', ha='center', va='bottom', fontsize=8)
    
    ax2.set_ylim(0.5, 1.5)
    ax2.set_ylabel('çœŸå®æœºåŠ¨')
    ax2.set_yticks([])
    ax2.grid(True, alpha=0.3, axis='x')
    ax2.legend()
    
    # å­å›¾3ï¼šé¢„æµ‹æœºåŠ¨æ—¶é—´è½´
    ax3 = axes2[2]
    if predictions is not None and len(predictions) > 0:
        y_pos = [1] * len(predictions)
        ax3.scatter(predictions, y_pos, color='green', s=100, marker='^', label='é¢„æµ‹æœºåŠ¨')
        for i, pred_time in enumerate(predictions[:20]):  # åªæ ‡è®°å‰20ä¸ª
            ax3.text(pred_time, 1.1, f'{i+1}', ha='center', va='bottom', fontsize=8)
    
    ax3.set_ylim(0.5, 1.5)
    ax3.set_ylabel('é¢„æµ‹æœºåŠ¨')
    ax3.set_xlabel('æ—¶é—´')
    ax3.set_yticks([])
    ax3.grid(True, alpha=0.3, axis='x')
    ax3.legend()
    
    # è®¾ç½®ç›¸åŒçš„xè½´èŒƒå›´
    for ax in axes2:
        ax.set_xlim(enhanced_data.index[0], enhanced_data.index[-1])
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾2
    output_path2 = os.path.join(output_dir, 'maneuver_detection_comparison.png')
    plt.savefig(output_path2, dpi=300, bbox_inches='tight')
    print(f"   å·²ä¿å­˜åˆ°: {output_path2}")
    plt.close()
    
    # é¢å¤–ï¼šæ¼‚ç§»å‹åŠ›åˆ†æå›¾
    if detector is not None:
        fig3, ax = plt.subplots(1, 1, figsize=(15, 5))
        
        if hasattr(detector, '_calculate_drift_pressure'):
            drift_pressure = detector._calculate_drift_pressure(enhanced_data)
            ax.plot(enhanced_data.index, drift_pressure, 
                   'orange', alpha=0.7, linewidth=1.5, label='æ¼‚ç§»å‹åŠ›')
            ax.axhline(y=detector.pressure_threshold, color='red', 
                      linestyle='--', alpha=0.5, label=f'å‹åŠ›é˜ˆå€¼: {detector.pressure_threshold}')
            
            # é«˜å‹åŠ›åŒºåŸŸå¡«å……
            ax.fill_between(enhanced_data.index, 0, drift_pressure, 
                           where=drift_pressure > detector.pressure_threshold,
                           color='red', alpha=0.2, label='é«˜å‹åŠ›æœŸ')
            
            # æ ‡è®°æœºåŠ¨äº‹ä»¶
            for m_time in maneuver_times:
                if enhanced_data.index[0] <= m_time <= enhanced_data.index[-1]:
                    ax.axvline(m_time, color='red', alpha=0.3, linestyle='--')
        
        ax.set_ylabel('æ¼‚ç§»å‹åŠ›')
        ax.set_xlabel('æ—¶é—´')
        ax.set_title('ç´¯ç§¯æ¼‚ç§»å‹åŠ›åˆ†æ')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        output_path3 = os.path.join(output_dir, 'drift_pressure_analysis.png')
        plt.savefig(output_path3, dpi=300, bbox_inches='tight')
        print(f"   å·²ä¿å­˜åˆ°: {output_path3}")
        plt.close()

# =====================================================================
# ä¸»å‡½æ•°
# =====================================================================
def main():
    """ä¸»å‡½æ•°"""
    
    # 1. åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨ç°æœ‰loaderï¼‰
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    from data.loader import SatelliteDataLoader
    
    loader = SatelliteDataLoader(data_dir="data")
    tle_data, maneuver_times = loader.load_satellite_data("Fengyun-4A")
    
    if tle_data is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    # 2. åˆ›å»ºæ¼‚ç§»å¢å¼ºç‰¹å¾
    enhanced_data = create_drift_enhanced_features(tle_data)
    if enhanced_data is None:
        print("âŒ ç‰¹å¾åˆ›å»ºå¤±è´¥")
        return
    
    # 3. è¿è¡Œæ”¹è¿›çš„æ£€æµ‹
    predictions, scores, test_data, detector, drift_analysis = run_improved_detection(
        enhanced_data, maneuver_times
    )
    
    # 4. è¯„ä¼°æ€§èƒ½
    if predictions is not None:
        metrics = evaluate_detection(predictions, test_data, maneuver_times)
        
        # 5. å¯è§†åŒ–ç»“æœ
        visualize_drift_detection(
            test_data, predictions, maneuver_times, 
            drift_analysis, detector
        )
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        generate_report(tle_data, enhanced_data, test_data, 
                       predictions, metrics, drift_analysis)
    
    print("\nâœ… æ”¹è¿›æ£€æµ‹ä»»åŠ¡å®Œæˆï¼")

def generate_report(tle_data, enhanced_data, test_data, 
                   predictions, metrics, drift_analysis):
    """ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
    print("\nğŸ“„ ç”ŸæˆæŠ¥å‘Š...")
    
    output_dir = 'outputs/fy4a_improved'
    os.makedirs(output_dir, exist_ok=True)
    report_path = os.path.join(output_dir, 'detection_report.txt')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("é£äº‘4A æ¼‚ç§»æ„ŸçŸ¥æœºåŠ¨æ£€æµ‹æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        f.write("1. æ•°æ®æ¦‚å†µ:\n")
        f.write(f"   åŸå§‹TLEè®°å½•: {len(tle_data)}\n")
        f.write(f"   å¢å¼ºç‰¹å¾æ•°: {len(enhanced_data.columns)}\n")
        f.write(f"   æµ‹è¯•é›†å¤§å°: {len(test_data)}\n")
        f.write(f"   æ—¶é—´èŒƒå›´: {test_data.index[0]} è‡³ {test_data.index[-1]}\n")
        
        f.write(f"\n2. æ£€æµ‹ç»“æœ:\n")
        if predictions is not None and len(predictions) > 0:
            f.write(f"   æ£€æµ‹å¼‚å¸¸æ•°: {len(predictions)}\n")
            f.write(f"   å¼‚å¸¸æ¯”ä¾‹: {len(predictions)/len(test_data)*100:.2f}%\n")
        else:
            f.write(f"   æ£€æµ‹å¼‚å¸¸æ•°: 0\n")
            f.write(f"   å¼‚å¸¸æ¯”ä¾‹: 0.00%\n")
        
        if metrics:
            f.write(f"\n3. æ€§èƒ½æŒ‡æ ‡:\n")
            f.write(f"   ç²¾ç¡®ç‡: {metrics['precision']:.3f}\n")
            f.write(f"   å¬å›ç‡: {metrics['recall']:.3f}\n")
            f.write(f"   F1åˆ†æ•°: {metrics['f1']:.3f}\n")
            f.write(f"   çœŸæ­£ä¾‹(TP): {metrics['tp']}\n")
            f.write(f"   å‡æ­£ä¾‹(FP): {metrics['fp']}\n")
            f.write(f"   å‡è´Ÿä¾‹(FN): {metrics['fn']}\n")
            f.write(f"   çœŸè´Ÿä¾‹(TN): {metrics['tn']}\n")
        
        if drift_analysis:
            f.write(f"\n4. æ¼‚ç§»åˆ†æ:\n")
            f.write(f"   å¹³å‡æ¼‚ç§»å‹åŠ›: {drift_analysis['mean_pressure']:.3f}\n")
            f.write(f"   æœ€å¤§æ¼‚ç§»å‹åŠ›: {drift_analysis['max_pressure']:.3f}\n")
            f.write(f"   é«˜å‹åŠ›æ—¶é—´æ¯”ä¾‹: {drift_analysis['high_pressure_ratio']*100:.1f}%\n")
            f.write(f"   å‹åŠ›è¶‹åŠ¿: {drift_analysis['pressure_trend']:.3f}\n")
            
            if drift_analysis['high_pressure_periods']:
                f.write(f"\n   é«˜å‹åŠ›æœŸé—´:\n")
                for i, period in enumerate(drift_analysis['high_pressure_periods'][:5]):
                    f.write(f"     {i+1}. {period['start']} è‡³ {period['end']} ")
                    f.write(f"({period['duration_days']}å¤©, æœ€å¤§å‹åŠ›: {period['max_pressure']:.3f})\n")
        
        f.write(f"\n5. æ£€æµ‹æ—¶é—´åˆ—è¡¨:\n")
        if predictions is not None and len(predictions) > 0:
            f.write(f"   å…±æ£€æµ‹åˆ° {len(predictions)} ä¸ªå¼‚å¸¸\n")
            for i, pred_time in enumerate(predictions[:20]):
                f.write(f"   {i+1:2d}. {pred_time}\n")
            if len(predictions) > 20:
                f.write(f"   ... (è¿˜æœ‰ {len(predictions)-20} ä¸ª)\n")
        else:
            f.write("   æ— æ£€æµ‹ç»“æœ\n")
    
    print(f"   æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

if __name__ == "__main__":
    main()