# scripts/test_particle_filter_performance.py
"""
å«æ˜ŸæœºåŠ¨æ£€æµ‹ç²’å­æ»¤æ³¢å™¨ï¼ˆParticle Filterï¼‰æ€§èƒ½æµ‹è¯•è„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨è¯„ä¼° `ParticleFilterDetector` åœ¨æ— ç›‘ç£æ¨¡å¼ä¸‹çš„æ€§èƒ½ã€‚
æµç¨‹:
1. åŠ è½½æ•°æ® (TLE å’ŒçœŸå®æœºåŠ¨æ—¥å¿—)ã€‚
2. ä½¿ç”¨ `MeanElementsProcessor` å‡†å¤‡å¹³æ ¹æ•°æ•°æ®ã€‚
3. å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
4. åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒç²’å­æ»¤æ³¢å™¨æ£€æµ‹å™¨ (ä¼°è®¡å™ªå£°åæ–¹å·®)ã€‚
5. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡ŒæœºåŠ¨æ£€æµ‹ã€‚
6. å¯¹æ¯”çœŸå®æœºåŠ¨æ—¥å¿—ï¼Œè¯„ä¼°æ£€æµ‹æ€§èƒ½ã€‚
7. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’Œæ€§èƒ½æŠ¥å‘Šã€‚
"""
import os
import sys
import pandas as pd
import numpy as np
from datetime import timedelta
import matplotlib.pyplot as plt
import json
import warnings

# --- é¡¹ç›®è·¯å¾„è®¾ç½® ---
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
from data.loader import SatelliteDataLoader
from data.mean_elements_processor import MeanElementsProcessor
from models.unsupervised.particle_filter import ParticleFilterDetector

warnings.filterwarnings('ignore', category=UserWarning) # å¿½ç•¥ä¸€äº›éå…³é”®è­¦å‘Š

class ParticleFilterExperiment:
    """å°è£…ç²’å­æ»¤æ³¢å™¨æ€§èƒ½æµ‹è¯•çš„å®éªŒç±»"""

    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–å®éªŒ
        Args:
            config (dict): åŒ…å«æ‰€æœ‰å®éªŒå‚æ•°çš„é…ç½®å­—å…¸ã€‚
        """
        self.config = config
        self.loader = SatelliteDataLoader(data_dir=config.get('data_dir', 'data'))
        self.processor = MeanElementsProcessor(satellite_type='auto', outlier_detection=True)
        self.detector = None
        self.results = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config['output_dir'], exist_ok=True)
        print(f"ğŸš€ åˆå§‹åŒ–ç²’å­æ»¤æ³¢å™¨å®éªŒï¼Œè¾“å‡ºå°†ä¿å­˜è‡³: {self.config['output_dir']}")

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹"""
        print("\n" + "="*60)
        print(f"ğŸ›°ï¸  å¼€å§‹å¯¹å«æ˜Ÿ '{self.config['satellite_name']}' è¿›è¡Œç²’å­æ»¤æ³¢å™¨æ€§èƒ½æµ‹è¯•")
        print("="*60)

        # 1. åŠ è½½æ•°æ®
        tle_data, maneuver_times = self._load_data()
        
        # 2. æ ¸å¿ƒæ•°æ®å¤„ç†: ä½¿ç”¨MeanElementsProcessorå‡†å¤‡æ•°æ®
        processed_data = self._process_data(tle_data)
        
        print("\n[*] æ ¹æ®é…ç½®ç­›é€‰è¾“å…¥æ•°æ®...")
        
        if self.config.get('use_only_mean_motion', False):
            # å¦‚æœå¼€å…³ä¸ºTrueï¼Œåªä½¿ç”¨ 'mean_motion'
            print("   -> 'use_only_mean_motion' å¼€å…³å¼€å¯ï¼Œæ¨¡å‹å°†åªä½¿ç”¨ [mean_motion] ç»´åº¦ã€‚")
            pf_input_data = processed_data[['mean_motion']]
        else:
            # å¦åˆ™ï¼Œä½¿ç”¨å¤šç»´æ ¸å¿ƒæ ¹æ•°
            print("   -> 'use_only_mean_motion' å…³é—­ï¼Œä½¿ç”¨å¤šç»´æ ¸å¿ƒæ ¹æ•°ã€‚")
            core_elements = [
                'mean_motion', 'eccentricity', 'inclination', 
                'arg_perigee', 'raan', 'mean_anomaly'
            ]
            available_core_elements = [col for col in core_elements if col in processed_data.columns]
            print(f"   -> æ¨¡å‹å°†ä½¿ç”¨ä»¥ä¸‹åˆ—è¿›è¡Œæ»¤æ³¢: {available_core_elements}")
            pf_input_data = processed_data[available_core_elements]

    

        # 3. æ•°æ®åˆ’åˆ†
        train_data, test_data = self._split_data(pf_input_data)
        
        # 4. è®­ç»ƒæ£€æµ‹å™¨ (æ— ç›‘ç£)
        self._train_detector(train_data)
        
        # 5. åœ¨æµ‹è¯•é›†ä¸Šæ£€æµ‹å¼‚å¸¸
        detected_anomalies, anomaly_scores = self._detect_anomalies(test_data)
        
        # 6. è¯„ä¼°ç»“æœ
        metrics = self._evaluate(detected_anomalies, test_data, maneuver_times)
        
        # 7. å¯è§†åŒ–å’ŒæŠ¥å‘Š
        self._visualize_and_report(processed_data, test_data, detected_anomalies, anomaly_scores, maneuver_times, metrics)
        
        print("\nâœ… å®éªŒå®Œæˆï¼")
        return self.results

    def _load_data(self):
        """åŠ è½½TLEæ•°æ®å’ŒæœºåŠ¨æ—¥å¿—"""
        print("\n[1/7] ğŸ“‚ åŠ è½½æ•°æ®...")
        tle_data, maneuver_times = self.loader.load_satellite_data(self.config['satellite_name'])
    
        # éªŒè¯æ•°æ®
        print(f"   -> TLEæ•°æ®å½¢çŠ¶: {tle_data.shape}")
        print(f"   -> TLEæ•°æ®åˆ—: {list(tle_data.columns[:10])}...")  # æ˜¾ç¤ºå‰10åˆ—
    
        # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['mean_motion', 'eccentricity', 'inclination', 'mean_anomaly']
        missing_cols = [col for col in required_cols if col not in tle_data.columns]
        if missing_cols:
            print(f"   -> âš ï¸ è­¦å‘Šï¼šç¼ºå°‘å…³é”®åˆ—: {missing_cols}")
    
        print(f"   -> åŠ è½½äº† {len(tle_data)} æ¡TLEè®°å½•")
        print(f"   -> åŠ è½½äº† {len(maneuver_times)} ä¸ªçœŸå®æœºåŠ¨äº‹ä»¶")
    
        # æ˜¾ç¤ºæ—¶é—´èŒƒå›´
        if not tle_data.empty:
            print(f"   -> TLEæ—¶é—´èŒƒå›´: {tle_data['epoch'].min()} åˆ° {tle_data['epoch'].max()}")
        if maneuver_times:
            print(f"   -> æœºåŠ¨æ—¶é—´èŒƒå›´: {min(maneuver_times)} åˆ° {max(maneuver_times)}")
    
        self.results['total_tle_records'] = len(tle_data)
        self.results['total_maneuvers'] = len(maneuver_times)
        return tle_data, maneuver_times

    def _process_data(self, tle_data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨MeanElementsProcessorå¤„ç†æ•°æ®ï¼Œä¸ºç²’å­æ»¤æ³¢å™¨å‡†å¤‡è¾“å…¥"""
        print("\n[2/7] ğŸ› ï¸  å¤„ç†å¹³æ ¹æ•°æ•°æ®...")
        processed_data = self.processor.process_tle_data(tle_data, self.config['satellite_name'])
        print(self.processor.get_processing_report())
        
        # æ£€æŸ¥æ¯åˆ—çš„NaNæƒ…å†µ
        print("\n   -> æ£€æŸ¥æ•°æ®è´¨é‡:")
        for col in processed_data.columns:
            nan_count = processed_data[col].isna().sum()
            nan_percent = (nan_count / len(processed_data)) * 100 if len(processed_data) > 0 else 0
            if nan_percent > 0:
                print(f"      - {col}: {nan_count} NaNå€¼ ({nan_percent:.1f}%)")
    
        # å¦‚æœeccentricityå…¨æ˜¯NaNï¼Œå°è¯•ä»åŸå§‹æ•°æ®æ¢å¤
        if 'eccentricity' in processed_data.columns and processed_data['eccentricity'].isna().all():
            print("   -> âš ï¸ eccentricityåˆ—å…¨æ˜¯NaNï¼Œå°è¯•ä»åŸå§‹æ•°æ®æ¢å¤...")
            if 'eccentricity' in tle_data.columns:
                # Ensure the length matches for direct assignment
                if len(tle_data) >= len(processed_data):
                     processed_data['eccentricity'] = tle_data['eccentricity'].values[:len(processed_data)]
                     print(f"      - æ¢å¤åçš„eccentricityèŒƒå›´: [{processed_data['eccentricity'].min():.6f}, {processed_data['eccentricity'].max():.6f}]")
                else:
                    print("     -> æ¢å¤å¤±è´¥ï¼šåŸå§‹TLEæ•°æ®é•¿åº¦å°äºå¤„ç†åæ•°æ®")

        # ç§»é™¤å…¨æ˜¯NaNçš„åˆ—
        cols_before = len(processed_data.columns)
        processed_data = processed_data.dropna(axis=1, how='all')
        cols_after = len(processed_data.columns)
        if cols_before > cols_after:
            print(f"   -> ç§»é™¤äº† {cols_before - cols_after} ä¸ªå…¨æ˜¯NaNçš„åˆ—")
    
        # å¡«å……å‰©ä½™çš„NaNå€¼
        print("   -> æ­£åœ¨å¡«å……æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿçš„NaNå€¼...")
        processed_data.ffill(inplace=True)
        processed_data.bfill(inplace=True)
    
        if processed_data.isnull().values.any():
            warnings.warn("æ•°æ®å¡«å……åä»å­˜åœ¨NaNå€¼ï¼Œå°†ä½¿ç”¨0å¡«å……å‰©ä½™ç©ºå€¼ã€‚")
            processed_data.fillna(0, inplace=True)
    
        print(f"   -> å¤„ç†åå¾—åˆ° {len(processed_data)} æ¡æœ‰æ•ˆæ•°æ®")
        print(f"   -> æœ€ç»ˆä½¿ç”¨çš„åˆ—: {list(processed_data.columns)}")
    
        return processed_data

    def _split_data(self, data: pd.DataFrame) -> tuple:
        """æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        print("\n[3/7] ğŸ“Š åˆ’åˆ†æ•°æ®é›†...")
        split_ratio = self.config['train_split_ratio']
        split_index = int(len(data) * split_ratio)
        
        train_data = data.iloc[:split_index]
        test_data = data.iloc[split_index:]
        
        print(f"   -> è®­ç»ƒé›†: {len(train_data)} æ¡ ({split_ratio:.0%})")
        print(f"   -> æµ‹è¯•é›†: {len(test_data)} æ¡ ({1-split_ratio:.0%})")
        return train_data, test_data

    def _train_detector(self, train_data: pd.DataFrame):
        """åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆç²’å­æ»¤æ³¢å™¨æ£€æµ‹å™¨"""
        print("\n[4/7] ğŸ§  è®­ç»ƒç²’å­æ»¤æ³¢å™¨æ£€æµ‹å™¨ (æ— ç›‘ç£)...")
    
        # é¦–å…ˆç”¨ä¸€ä¸ªä¸´æ—¶æ£€æµ‹å™¨æ¥ä¼°è®¡åˆ†æ•°åˆ†å¸ƒ
        temp_detector = ParticleFilterDetector(
            n_particles=100,  # ç”¨è¾ƒå°‘çš„ç²’å­å¿«é€Ÿä¼°è®¡
            anomaly_threshold=float('inf'), # è®¾ç½®ä¸ºæ— ç©·å¤§ä»¥è·å–æ‰€æœ‰åˆ†æ•°
            noise_scaling_factor=self.config.get('noise_scaling_factor', 1.0),
            auto_tune_threshold=False
        )
        temp_detector.fit(train_data)
    
        # åœ¨è®­ç»ƒæ•°æ®ä¸Šè®¡ç®—åˆ†æ•°ä»¥äº†è§£åˆ†å¸ƒ
        _, train_scores = temp_detector.detect_anomalies(train_data)
    
        actual_threshold = self.config.get('anomaly_threshold', 12)
        
        if len(train_scores) > 0:
            # åŸºäºåˆ†æ•°åˆ†å¸ƒä¼°è®¡é˜ˆå€¼
            percentile = self.config.get('threshold_percentile', 95.0)
            estimated_threshold = np.percentile(train_scores, percentile)

            mean_score = np.mean(train_scores)
            std_score = np.std(train_scores)
            k_factor = self.config.get('threshold_std_dev_factor', 3.0) 
            
            estimated_threshold = mean_score + k_factor * std_score            
            print(f"   -> è®­ç»ƒé›†åˆ†æ•°ç»Ÿè®¡:")
            print(f"      - èŒƒå›´: [{np.min(train_scores):.2f}, {np.max(train_scores):.2f}]")
            print(f"      - å‡å€¼: {np.mean(train_scores):.2f}")
            print(f"      - æ ‡å‡†å·®: {np.std(train_scores):.2f}")
            print(f"      - {percentile}%ç™¾åˆ†ä½æ•°: {estimated_threshold:.2f}")
    
            # ä½¿ç”¨ä¼°è®¡çš„é˜ˆå€¼ï¼Œé™¤éé…ç½®ä¸­æŒ‡å®šäº†åˆç†çš„å€¼ (ä¾‹å¦‚ï¼Œä¸€ä¸ªè´Ÿæ•°)
            # è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼è§„åˆ™ï¼šå¦‚æœç”¨æˆ·è®¾ç½®äº†ä¸€ä¸ªçœ‹ä¼¼éšæ„çš„æ­£å€¼ï¼Œæˆ‘ä»¬å®æ„¿ç›¸ä¿¡è‡ªé€‚åº”é˜ˆå€¼
            if self.config.get('use_auto_threshold', True):
                actual_threshold = estimated_threshold
                print(f"   -> 'use_auto_threshold' å·²å¼€å¯, ä½¿ç”¨è‡ªåŠ¨ä¼°ç®—çš„é˜ˆå€¼: {actual_threshold:.2f}")
                print(f"   -> ä½¿ç”¨é…ç½®çš„é˜ˆå€¼: {actual_threshold:.2f}")
            else:
                print(f"   -> 'use_auto_threshold' å·²å…³é—­, ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é˜ˆå€¼: {actual_threshold:.2f}")
        else:
            print(f"   -> æ— æ³•è®¡ç®—è®­ç»ƒé›†åˆ†æ•°ï¼Œä½¿ç”¨é…ç½®çš„é˜ˆå€¼: {actual_threshold:.2f}")
        
        # åˆ›å»ºæœ€ç»ˆçš„æ£€æµ‹å™¨
        self.detector = ParticleFilterDetector(
            n_particles=self.config.get('n_particles', 250),
            anomaly_threshold=actual_threshold,
            noise_scaling_factor=self.config.get('noise_scaling_factor', 1.0),
            auto_tune_threshold=False # ä¸¥æ ¼æ— ç›‘ç£
        )
    
        # è®­ç»ƒæ¨¡å‹
        self.detector.fit(train_data)
        print("   -> åæ–¹å·®çŸ©é˜µ Q å’Œ R å·²ä»è®­ç»ƒæ•°æ®ä¸­ä¼°è®¡å®Œæˆã€‚")
        print(f"   -> æ¨¡å‹æ‘˜è¦: {self.detector.get_model_summary()}")


    def _detect_anomalies(self, test_data: pd.DataFrame) -> tuple:
        """åœ¨æµ‹è¯•æ•°æ®ä¸Šæ‰§è¡Œå¼‚å¸¸æ£€æµ‹"""
        print("\n[5/7] ğŸ” åœ¨æµ‹è¯•é›†ä¸Šæ£€æµ‹æœºåŠ¨...")
        detected_anomalies, anomaly_scores = self.detector.detect_anomalies(test_data)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if len(anomaly_scores) > 0:
            print(f"   -> å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡:")
            print(f"      - æœ€å°å€¼: {np.min(anomaly_scores):.2f}")
            print(f"      - æœ€å¤§å€¼: {np.max(anomaly_scores):.2f}")
            print(f"      - å¹³å‡å€¼: {np.mean(anomaly_scores):.2f}")
            print(f"      - æ ‡å‡†å·®: {np.std(anomaly_scores):.2f}")
        print(f"      - é˜ˆå€¼: {self.detector.anomaly_threshold:.2f}")
    
        # æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„æ—¶é—´ç‚¹
        if len(anomaly_scores) > 10:
            top_scores_idx = np.argsort(anomaly_scores)[-10:]
            print(f"   -> å¾—åˆ†æœ€é«˜çš„10ä¸ªæ—¶é—´ç‚¹:")
            # score_index starts from the second element of test_data
            score_index_map = test_data.index[1:]
            for idx in top_scores_idx:
                if idx < len(score_index_map):
                    timestamp = score_index_map[idx]
                    score = anomaly_scores[idx]
                    print(f"      {timestamp}: {score:.2f}")        
        
        print(f"   -> æ£€æµ‹åˆ° {len(detected_anomalies)} ä¸ªæ½œåœ¨æœºåŠ¨äº‹ä»¶ã€‚")
        self.results['detected_anomalies_count'] = len(detected_anomalies)
        return detected_anomalies, anomaly_scores

    def _evaluate(self, detected_anomalies: list, test_data: pd.DataFrame, maneuver_times: list) -> dict:
        """è¯„ä¼°æ£€æµ‹æ€§èƒ½"""
        print("\n[6/7] ğŸ“ˆ è¯„ä¼°æ£€æµ‹æ€§èƒ½...")
    
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæµ‹è¯•é›†çš„æ—¶é—´èŒƒå›´
        print(f"   -> æµ‹è¯•é›†æ—¶é—´èŒƒå›´: {test_data.index.min()} åˆ° {test_data.index.max()}")
    
        # æ˜¾ç¤ºæµ‹è¯•é›†ä¸­çš„çœŸå®æœºåŠ¨
        true_maneuvers_in_test = []
        window = timedelta(days=self.config.get('label_window_days', 1))
    
        for m_time in maneuver_times:
            if test_data.index.min() <= m_time <= test_data.index.max():
                true_maneuvers_in_test.append(m_time)
    
        print(f"   -> æµ‹è¯•é›†ä¸­çš„çœŸå®æœºåŠ¨æ—¶é—´ (å‰5ä¸ª):")
        if not true_maneuvers_in_test:
            print("      æ— ")
        for i, m_time in enumerate(true_maneuvers_in_test[:5]):
            print(f"      {i+1}. {m_time}")
    
        # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„å¼‚å¸¸
        if detected_anomalies:
            print(f"   -> æ£€æµ‹åˆ°çš„å¼‚å¸¸æ—¶é—´ (å‰5ä¸ª):")
            for i, d_time in enumerate(detected_anomalies[:5]):
                print(f"      {i+1}. {d_time}")
                # æ£€æŸ¥æœ€è¿‘çš„çœŸå®æœºåŠ¨
                if true_maneuvers_in_test:
                    closest_maneuver = min(true_maneuvers_in_test,
                                          key=lambda x: abs((x - d_time).total_seconds()))
                    distance_days = abs((closest_maneuver - d_time).total_seconds()) / 86400
                    print(f"         æœ€è¿‘çš„çœŸå®æœºåŠ¨è·ç¦»: {distance_days:.1f} å¤©")
        else:
            print("   -> æœªæ£€æµ‹åˆ°ä»»ä½•å¼‚å¸¸ã€‚")
            
        # åˆ›å»ºçœŸå®æ ‡ç­¾
        true_labels = pd.Series(0, index=test_data.index)
        true_maneuver_in_test_count = 0
    
        for m_time in maneuver_times:
            if test_data.index.min() <= m_time <= test_data.index.max():
                true_maneuver_in_test_count += 1
                mask = (true_labels.index >= m_time - window) & (true_labels.index <= m_time + window)
                true_labels[mask] = 1
    
        # åˆ›å»ºé¢„æµ‹æ ‡ç­¾
        pred_labels = pd.Series(0, index=test_data.index)
        if detected_anomalies:
            valid_anomalies = [ts for ts in detected_anomalies if ts in pred_labels.index]
            if valid_anomalies:
                pred_labels.loc[valid_anomalies] = 1
    
        # è®¡ç®—æ··æ·†çŸ©é˜µå…ƒç´ 
        tp = ((pred_labels == 1) & (true_labels == 1)).sum()
        fp = ((pred_labels == 1) & (true_labels == 0)).sum()
        fn = ((pred_labels == 0) & (true_labels == 1)).sum()
        tn = ((pred_labels == 0) & (true_labels == 0)).sum()
    
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
        metrics = {
            'TP': int(tp),
            'FP': int(fp),
            'FN': int(fn),
            'TN': int(tn),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1)
        }
    
        print(f"   -> æµ‹è¯•é›†ä¸­çš„çœŸå®æœºåŠ¨äº‹ä»¶æ•°: {true_maneuver_in_test_count}")
        print(f"   -> å¸¦æ ‡ç­¾çš„æ—¶é—´ç‚¹æ•° (çª—å£={window.days*2}å¤©): {(true_labels == 1).sum()}")
        print(f"   -> æ€§èƒ½æŒ‡æ ‡:")
        print(f"      - ç²¾ç¡®ç‡ (Precision): {precision:.3f}")
        print(f"      - å¬å›ç‡ (Recall):    {recall:.3f}")
        print(f"      - F1åˆ†æ•° (F1-Score):   {f1:.3f}")
        print(f"      - TP: {tp}, FP: {fp}, FN: {fn}")
    
        self.results['performance_metrics'] = metrics
        return metrics

    def _visualize_and_report(self, all_data, test_data, anomalies, scores, maneuver_times, metrics):
        """ç”Ÿæˆå¹¶ä¿å­˜å›¾è¡¨å’ŒæŠ¥å‘Š"""
        print("\n[7/7] ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’ŒæŠ¥å‘Š...")

        # --- å¯è§†åŒ– ---
        fig, axes = plt.subplots(2, 1, figsize=(18, 12), sharex=True)
        fig.suptitle(f"Particle Filter Maneuver Detection for {self.config['satellite_name']}", fontsize=16)

        # å›¾1: Mean Motion å’Œæ£€æµ‹ç»“æœ
        ax1 = axes[0]
        ax1.plot(all_data.index, all_data['mean_motion'], color='gray', alpha=0.5, label='Mean Motion (All Data)')
        ax1.plot(test_data.index, test_data['mean_motion'], 'b-', label='Mean Motion (Test Set)')
        
        # æ ‡è®°çœŸå®æœºåŠ¨
        true_man_in_test = [m for m in maneuver_times if test_data.index.min() <= m <= test_data.index.max()]
        if true_man_in_test:
             ax1.vlines(true_man_in_test, ymin=ax1.get_ylim()[0], ymax=ax1.get_ylim()[1], color='green', linestyle='--', label='True Maneuver', lw=2)
        
        # æ ‡è®°æ£€æµ‹åˆ°çš„æœºåŠ¨
        if anomalies:
            valid_anomalies = [a for a in anomalies if a in test_data.index]
            if valid_anomalies:
                anomaly_values = test_data.loc[valid_anomalies]['mean_motion']
                ax1.scatter(valid_anomalies, anomaly_values, color='red', s=80, marker='^', label='Detected Maneuver', zorder=5)
        
        ax1.set_ylabel("Mean Motion (rev/day)")
        ax1.set_title("Maneuver Detection Results")
        ax1.legend()
        ax1.grid(True, which='both', linestyle='--', linewidth=0.5)

        # å›¾2: å¼‚å¸¸åˆ†æ•°å’Œé˜ˆå€¼
        ax2 = axes[1]
        # å¼‚å¸¸åˆ†æ•°æ˜¯ä»ç¬¬äºŒä¸ªæ—¶é—´ç‚¹å¼€å§‹çš„ï¼Œæ‰€ä»¥éœ€è¦å¯¹é½
        if len(scores) > 0:
            score_index = test_data.index[1:len(scores)+1]
            ax2.plot(score_index, scores, 'r-', alpha=0.8, label='Anomaly Score')
            ax2.axhline(y=self.detector.anomaly_threshold, color='black', linestyle=':', label=f"Threshold ({self.detector.anomaly_threshold:.2f})", lw=2)
            ax2.set_yscale('log') # ä½¿ç”¨å¯¹æ•°åæ ‡è½´ä»¥ä¾¿è§‚å¯Ÿ

        
        ax2.set_xlabel("Time")
        ax2.set_ylabel("Anomaly Score (Negative Log-Likelihood)")
        ax2.set_title("Anomaly Scores Over Time")
        ax2.legend()
        ax2.grid(True, which='both', linestyle='--', linewidth=0.5)
        
        fig_path = os.path.join(self.config['output_dir'], f"{self.config['satellite_name']}_pf_detection_results.png")
        plt.savefig(fig_path, dpi=300)
        plt.close(fig)
        print(f"   -> å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {fig_path}")

        # --- ç”ŸæˆæŠ¥å‘Š ---
        report = {
            "experiment_config": self.config,
            "data_summary": {
                "total_tle_records": self.results.get('total_tle_records'),
                "total_maneuvers": self.results.get('total_maneuvers'),
                "processed_records": len(all_data),
                "train_set_size": len(test_data.iloc[:int(len(test_data)*self.config['train_split_ratio'])])
            },
            "model_summary": self.detector.get_model_summary(),
            "performance_metrics": metrics,
            "detected_anomaly_timestamps": [ts.isoformat() for ts in anomalies]
        }
        
        report_path = os.path.join(self.config['output_dir'], f"{self.config['satellite_name']}_pf_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=4)
        
        print(f"   -> è¯¦ç»†JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

# =====================================================================
# ä¸»æ‰§è¡Œå‡½æ•°
# =====================================================================
def main():
    """ä¸»å‡½æ•°ï¼Œé…ç½®å¹¶è¿è¡Œå®éªŒ"""
    
    # --- å®éªŒé…ç½® ---
    # ä½ å¯ä»¥åœ¨è¿™é‡Œæ›´æ”¹å«æ˜Ÿåç§°å’Œç²’å­æ»¤æ³¢å™¨å‚æ•°
    experiment_config = {
        'satellite_name': 'Fengyun-4A', # å¯æ›´æ¢ä¸º: 'Fengyun-4A', 'Sentinel-3A', 'Jason-2' ç­‰
        'data_dir': 'data',
        'output_dir': 'outputs/particle_filter_test',
        
        # æ•°æ®åˆ’åˆ†é…ç½®
        'train_split_ratio': 0.7, # 70% çš„æ•°æ®ç”¨äºè®­ç»ƒ
        
        # ç²’å­æ»¤æ³¢å™¨æ£€æµ‹å™¨é…ç½®
        'n_particles': 1000,
        #'anomaly_threshold': 3,  # åˆå§‹å€¼ï¼Œå¯èƒ½ä¼šè¢«è‡ªé€‚åº”é˜ˆå€¼è¦†ç›–
        'threshold_std_dev_factor': 1, 

        'use_auto_threshold': True,      # æ–°å¢ï¼šæ˜ç¡®æ§åˆ¶æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨é˜ˆå€¼

        #'anomaly_threshold': 15.0, 

        'use_only_mean_motion': True, # æ˜¯å¦åªä½¿ç”¨å¹³å‡è¿åŠ¨è¿›è¡Œæ£€æµ‹ï¼ˆTrue/Falseï¼‰
        'noise_scaling_factor': 0.1,   # å¢åŠ å™ªå£°ä»¥æé«˜æ•æ„Ÿåº¦

        # è¯„ä¼°é…ç½®
        'label_window_days': 2.5, # å¢åŠ çª—å£å¤§å°ï¼ŒGEOå«æ˜ŸæœºåŠ¨å¯èƒ½æŒç»­è¾ƒé•¿
    }

    # è¿è¡Œå®éªŒ
    experiment = ParticleFilterExperiment(experiment_config)
    experiment.run()


if __name__ == "__main__":
    main()