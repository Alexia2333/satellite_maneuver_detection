# refactored_detector.py

import pandas as pd
import numpy as np
import xgboost as xgb
from datetime import timedelta

# =====================================================================
# ç‰¹å¾å·¥ç¨‹å‡½æ•° (ä»ä¸»è„šæœ¬è¿ç§»è¿‡æ¥ï¼Œä¿æŒç‹¬ç«‹)
# =====================================================================
def create_drift_enhanced_features(tle_data, scaling_factor, target_col='mean_motion'):
    print("\nğŸ› ï¸ åˆ›å»ºæ¼‚ç§»å¢å¼ºç‰¹å¾")
    print("-" * 20)
    
    data = tle_data.set_index('epoch').copy()
    base_cols = ['mean_motion', 'eccentricity', 'inclination']
    available_cols = [col for col in base_cols if col in data.columns]
    
    if not available_cols:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åŸºç¡€ç‰¹å¾åˆ—: {base_cols}")
        return None
    
    print(f"   åŸºç¡€å‚æ•°: {available_cols}")
    enhanced_data = data[available_cols].copy()
    
    lags = [1, 2, 3, 7]
    windows = [3, 7, 14]
    for col in available_cols:
        for lag in lags:
            enhanced_data[f'{col}_lag_{lag}'] = data[col].shift(lag)
        for window in windows:
            enhanced_data[f'{col}_rolling_mean_{window}'] = data[col].rolling(window, min_periods=window//2).mean()
            enhanced_data[f'{col}_rolling_std_{window}'] = data[col].rolling(window, min_periods=window//2).std()
        enhanced_data[f'{col}_diff_1'] = data[col].diff(1)
        enhanced_data[f'{col}_diff_7'] = data[col].diff(7)

    long_windows = [30, 60]
    for col in available_cols:
        for window in long_windows:
            long_mean = data[col].rolling(window, min_periods=window//3).mean()
            long_std = data[col].rolling(window, min_periods=window//3).std()
            enhanced_data[f'{col}_drift_from_{window}d'] = data[col] - long_mean
            enhanced_data[f'{col}_drift_zscore_{window}d'] = (data[col] - long_mean) / (long_std + 1e-8)

    trend_windows = [7, 14]
    for col in available_cols:
        for window in trend_windows:
            enhanced_data[f'{col}_trend_{window}d'] = data[col].rolling(window).apply(
                lambda x: (x[-1] - x[0]) / len(x) if len(x) > 1 else 0, raw=True)

    for col in available_cols:
        short_mean = data[col].rolling(7, min_periods=3).mean()
        short_drift = (data[col] - short_mean).abs()
        enhanced_data[f'{col}_cumulative_drift_7d'] = short_drift.rolling(7, min_periods=3).sum()
        enhanced_data[f'{col}_cumulative_drift_14d'] = short_drift.rolling(14, min_periods=7).sum()

    if len(available_cols) > 1:
        for window in [7, 14]:
            drift_cols = [f'{col}_drift_from_30d' for col in available_cols if f'{col}_drift_from_30d' in enhanced_data.columns]
            if drift_cols:
                enhanced_data[f'combined_drift_l2_{window}d'] = np.sqrt(enhanced_data[drift_cols].pow(2).sum(axis=1))

    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆå·®åˆ†ï¼‰
    mean_motion_diff = data[target_col].diff()
    enhanced_data['target'] = mean_motion_diff.shift(-1).abs()

    # âœ… æ— éœ€ç­›é€‰å°å€¼ï¼šä¿ç•™æ‰€æœ‰éNaNï¼Œç›´æ¥åšæ”¾å¤§å¤„ç†ï¼ˆé‡ç‚¹ï¼ï¼ï¼‰
    enhanced_data = enhanced_data.dropna(subset=['target'])

    # âœ… æ•°å€¼æ”¾å¤§ + log1pï¼Œæå‡æ•°å€¼å¯å­¦ä¹ æ€§
    enhanced_data['target'] = np.log1p(enhanced_data['target'] * 1e8)

    # æ‰“å°ç¡®è®¤
    print("âœ… åº”ç”¨äº† log1p(target * 1e8) å˜æ¢")
    print(enhanced_data['target'].describe())
    


    # å¡«å……å…¶ä»–ç‰¹å¾
    enhanced_data = enhanced_data.fillna(method='ffill', limit=3)
    enhanced_data = enhanced_data.fillna(method='bfill', limit=3)
    for col in enhanced_data.columns:
        if enhanced_data[col].isna().any():
            median_val = enhanced_data[col].median()
            enhanced_data[col].fillna(median_val if not pd.isna(median_val) else 0, inplace=True)

    print(f"âœ… ç‰¹å¾åˆ›å»ºå®Œæˆ")
    print(f"   ç‰¹å¾æ•°é‡: {len(enhanced_data.columns)}")
    print(f"   æœ‰æ•ˆè®°å½•: {len(enhanced_data)}")
    print(f"   ç›®æ ‡å˜é‡ç»Ÿè®¡: mean={enhanced_data['target'].mean():.6f}, std={enhanced_data['target'].std():.6f}")
    return enhanced_data


# =====================================================================
# æ—¶é—´åºåˆ—èšç±»å‡½æ•° (ä»ä¸»è„šæœ¬è¿ç§»è¿‡æ¥ï¼Œä¿æŒç‹¬ç«‹)
# =====================================================================
def group_and_filter_anomalies(timestamps, max_gap=pd.Timedelta(days=1), min_group_size=2): # <--- å¢åŠ  min_group_size å‚æ•°
    if timestamps.empty:
        return []

    timestamps = pd.Series(sorted(timestamps))
    groups = []
    current_group = [timestamps.iloc[0]]

    for i in range(1, len(timestamps)):
        if (timestamps.iloc[i] - timestamps.iloc[i-1]) <= max_gap:
            current_group.append(timestamps.iloc[i])
        else:
            if len(current_group) >= min_group_size:
                groups.extend(current_group)
            current_group = [timestamps.iloc[i]]

    if len(current_group) >= min_group_size:
        groups.extend(current_group)

    return sorted(list(set(groups)))
# =====================================================================
# æ ¸å¿ƒæ¢æµ‹å™¨ç±»
# =====================================================================
class DriftAwareDetector:
    def __init__(self, config: dict):
        """
        é€šè¿‡ä¸€ä¸ªé…ç½®å­—å…¸æ¥åˆå§‹åŒ–æ¢æµ‹å™¨
        """
        print("   åˆå§‹åŒ–æ¢æµ‹å™¨...")
        self.config = config
        self.model = None
        self.feature_names = None
        self.residual_threshold = None
        self.change_rate_threshold = None
        self.pressure_baseline = None

    def fit(self, train_data: pd.DataFrame):
        """
        è®­ç»ƒæ¨¡å‹å¹¶è®¡ç®—æ‰€æœ‰å¿…è¦çš„é˜ˆå€¼
        """
        print("   ğŸ”§ è®­ç»ƒXGBoostæ¨¡å‹å¹¶è®¡ç®—é˜ˆå€¼...")
        
        X_train = train_data.drop(columns=['target'])
        y_train = train_data['target']
        self.feature_names = list(X_train.columns)

        # åˆå§‹åŒ–å¹¶è®­ç»ƒXGBoostæ¨¡å‹
        self.model = xgb.XGBRegressor(**self.config['xgb_params'])
        self.model.fit(X_train, y_train)
        
        print(f"   è®­ç»ƒRÂ²: {self.model.score(X_train, y_train):.4f}")

        # è®¡ç®—è®­ç»ƒé›†æ®‹å·®å’Œå˜åŒ–ç‡
        y_train_pred = self.model.predict(X_train)
        train_residuals = np.abs(y_train - y_train_pred)
        train_change_rates = np.abs(y_train - y_train_pred) / (np.abs(y_train) + 1e-8)

        # è®¡ç®—å¹¶å­˜å‚¨é˜ˆå€¼
        self.residual_threshold = train_residuals.mean() + self.config['threshold_std_multiplier'] * train_residuals.std()
        self.change_rate_threshold = np.percentile(train_change_rates, self.config['threshold_percentile'])
        
        # è®¡ç®—å¹¶å­˜å‚¨æ¼‚ç§»å‹åŠ›åŸºçº¿
        drift_features = [col for col in self.feature_names if any(keyword in col for keyword in ['drift', 'cumulative'])]
        if drift_features:
            train_drift_pressure = train_data[drift_features].abs().mean(axis=1)
            self.pressure_baseline = np.median(train_drift_pressure)
        
        print(f"   æ®‹å·®é˜ˆå€¼: {self.residual_threshold:.6f}")
        print(f"   å˜åŒ–ç‡é˜ˆå€¼: {self.change_rate_threshold:.6f}")
        print(f"   å‹åŠ›åŸºçº¿: {self.pressure_baseline:.6f}" if self.pressure_baseline is not None else "   æ— å‹åŠ›ç‰¹å¾")

    def detect(self, test_data: pd.DataFrame) -> tuple:
        """
        åœ¨æµ‹è¯•é›†ä¸Šæ‰§è¡Œæˆ‘ä»¬æœ€ç»ˆä¼˜åŒ–çš„æ£€æµ‹é€»è¾‘
        """
        print("   ğŸ§ª åº”ç”¨å¢å¼ºæ£€æµ‹é€»è¾‘...")
        if self.model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•ã€‚")

        X_test = test_data[self.feature_names]
        y_test = test_data['target']
        y_test_pred = self.model.predict(X_test)

        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        test_residuals = np.abs(y_test - y_test_pred)
        test_change_rates = np.abs(y_test - y_test_pred) / (np.abs(y_test) + 1e-8)
        
        # è·¯å¾„A: "å¼ºä¿¡å·" æ£€æµ‹
        strong_signal_mask = (test_residuals > self.residual_threshold) & (test_change_rates > self.change_rate_threshold)
        print(f"      - å¼ºä¿¡å·æ£€æµ‹å‘ç°: {strong_signal_mask.sum()} ä¸ªå¼‚å¸¸")

        # è·¯å¾„B: "å‹åŠ›è¾…åŠ©ä¿¡å·" æ£€æµ‹
        pressure_assisted_mask = pd.Series(False, index=test_data.index)
        normalized_pressure = None
        drift_features = [col for col in self.feature_names if any(keyword in col for keyword in ['drift', 'cumulative'])]
        
        if drift_features and self.pressure_baseline is not None and self.pressure_baseline > 0:
            test_drift_pressure = test_data[drift_features].abs().mean(axis=1)
            normalized_pressure = test_drift_pressure / self.pressure_baseline
            
            for i in range(len(test_residuals)):
                if normalized_pressure.iloc[i] > self.config['pressure_activation_threshold']:
                    reduction_factor = 1.0 - self.config['pressure_reduction_factor'] * min((normalized_pressure.iloc[i] - self.config['pressure_activation_threshold']), 1.0)
                    adjusted_threshold = self.residual_threshold * reduction_factor
                    if test_residuals[i] > adjusted_threshold:
                        pressure_assisted_mask.iloc[i] = True
            print(f"      - å‹åŠ›è¾…åŠ©æ£€æµ‹å‘ç°: {pressure_assisted_mask.sum()} ä¸ªå¼‚å¸¸")

        # åˆå¹¶ç»“æœ
        final_anomaly_mask = strong_signal_mask | pressure_assisted_mask
        initial_anomaly_indices = np.where(final_anomaly_mask)[0]
        initial_anomaly_timestamps = test_data.index[initial_anomaly_indices]
        
        # è¿”å›åŸå§‹æ£€æµ‹ç»“æœåŠåˆ†ææ‰€éœ€æ•°æ®
        return initial_anomaly_timestamps, {
            'test_residuals': test_residuals,
            'normalized_pressure': normalized_pressure
        }